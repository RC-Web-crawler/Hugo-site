[{"uri":"https://rc-web-crawler.github.io/2nd_home/","title":"Content Analysis","tags":[],"description":"","content":"Refresh Corner: Content Analysis This is the webpage for the Content Analysis study groupüëª Team members: Aiwenüëßüèª, Mengyueüë©üèª, Shuyanüë©üèª‚Äçüíº, Songrenüë©üèª‚Äçüî¨, Weijiaüë©üèª‚Äçü¶∞, Yijieüë®üèª‚Äçüíª Content Analysis 2.1 Keyword Analysis 2.2 Content Analysis 2.3 eCRF Index Shiny App "},{"uri":"https://rc-web-crawler.github.io/2nd_home/5_keyword/","title":"Keyword Analysis","tags":[],"description":"","content":"Purpose =======\nIn this section, we will use text mining methods to derive information from the text of keywords. We explore the frequency, coverage, and the relationship between keywords, therefore identifying keywords which are important for our further analysis and model building.\nPre-prepare: install R packages and import data ===============================================\nlibrary(knitr) library(readxl) library(tidyverse) library(tidytext) library(igraph) library(ggraph) library(textstem) library(RSQLite) library(plotly) data \u0026lt;- read_excel(\u0026quot;~/Downloads//paper_keyword.xlsx\u0026quot;) Data transformation ===================\nKeyword cleaning We transform the data into a tibble (tibbles are a modern take on data frames) and add the row number with the column name ‚Äòdocument‚Äô. We clean the data by removing certain special characters and irrelevant information from the ‚Äòkeyword‚Äô column.\ndata2 \u0026lt;- as_tibble(data) %\u0026gt;% mutate(keyword = tolower(str_trim(sub(\u0026quot;((Pages|Size|\\\\().*)|(√Ç|¬Æ)\u0026quot;, \u0026quot;\u0026quot;, keyword), \u0026quot;left\u0026quot;)), document = row_number()) %\u0026gt;% select(document, title, keyword) kable(data2[1:10,]) document title keyword 1 MARKUP: The Power of Choice and Change code from github eric gebhart, sas institute inc. 2 A Tutorial on Reduced Error Logistic Regression updated jsm 2008 paper 3 The SAS Supervisor sas communities page 4 Introduction to the Macro Language macro 5 Unlimiting a Limited Macro Environment macro 6 The Right Approach to Learning PROC TABULATE tabulate 7 SAS Macro Environments: Local and Global macro 8 Introduction to the Macro Language macro 9 The Right Approach to Learning PROC TABULATE tabulate 10 Conquering the Dreaded Macro Error macro Tokenization we need to split the text into individual words (a process called tokenization) and transform it to a tidy data structure (i.e.¬†each word has its own row). To do this, we use tidytext‚Äôs unnest_tokens() function. We remove duplicates so that if a word appeared in a paper multiple times, it will be counted only once. We also remove keywords that contain numbers only (e.g.¬†years).\nStop words Often in text analysis, we will want to remove stop words: Stop words are words that are not useful for an analysis, typically extremely common words such as ‚Äúthe‚Äù, ‚Äúof‚Äù, ‚Äúto‚Äù, and so forth. We remove stop words in our data by using tidytext‚Äôs get_stopwords() function with an anti_join() from the package dplyr.\nLemmatization In our data, there are words such as ‚Äúmacro‚Äù and macros\u0026quot; that means the same but are in different inflected forms. In order to analyze them as a single item, we need to reduce words to their lemma form. Below is an example of lemmatizing ‚Äúbe‚Äù, using textstem‚Äôs lemmatize_words().\nbw \u0026lt;- c('are', 'am', 'being', 'been', 'be') lemmatize_words(bw) ## [1] \u0026quot;be\u0026quot; \u0026quot;be\u0026quot; \u0026quot;be\u0026quot; \u0026quot;be\u0026quot; \u0026quot;be\u0026quot; # tokenization kw \u0026lt;- data2 %\u0026gt;% unnest_tokens(oldword, keyword) %\u0026gt;% # lemmatizing words mutate(word = case_when(length(oldword) \u0026lt; 6 | oldword %in% c('ods','data','mining','learning') ~ oldword, TRUE ~ lemmatize_words(oldword))) %\u0026gt;% distinct() %\u0026gt;% # remove duplicates anti_join(get_stopwords()) %\u0026gt;% # stop words filter(is.na(as.numeric(word))) # remove numbers kw %\u0026gt;% filter(document==1|document==2) %\u0026gt;% kable() document title oldword word 1 MARKUP: The Power of Choice and Change code code 1 MARKUP: The Power of Choice and Change github github 1 MARKUP: The Power of Choice and Change eric eric 1 MARKUP: The Power of Choice and Change gebhart gebhart 1 MARKUP: The Power of Choice and Change sas sas 1 MARKUP: The Power of Choice and Change institute institute 1 MARKUP: The Power of Choice and Change inc inc 2 A Tutorial on Reduced Error Logistic Regression updated update 2 A Tutorial on Reduced Error Logistic Regression jsm jsm 2 A Tutorial on Reduced Error Logistic Regression paper paper Exploratory analysis ====================\nTerm frequency One measure of how important a word may be is its term frequency. Plot words with a frequency greater than 400:\nkw %\u0026gt;% count(word, sort = TRUE) %\u0026gt;% filter(n \u0026gt; 400) %\u0026gt;% mutate(word = reorder(word, n)) %\u0026gt;% ggplot(aes(word, n)) + geom_col(aes()) + xlab(NULL) + scale_y_continuous(expand = c(0, 0)) + coord_flip() + theme_classic(base_size = 12) + labs(title=\u0026quot;Word frequency\u0026quot;, subtitle=\u0026quot;n \u0026gt; 400\u0026quot;)+ theme(plot.title = element_text(lineheight=.8, face=\u0026quot;bold\u0026quot;)) + scale_fill_brewer() Keyword Coverage Analysis We want to analyze the ability of keywords to cover the articles for further analysis. First, sort keywords in descending order of frequency and give the count of keywords. There are total 1821 cleaned keywords.\nkw_clean \u0026lt;- read_excel(\u0026quot;~/Downloads//kw_clean.xlsx\u0026quot;) keyword \u0026lt;- count(kw_clean, word, sort = TRUE) keyword$keyword_count \u0026lt;- seq_len(nrow(keyword)) nrow(keyword) ## [1] 1821 Secondly, calculate each keyword can cover how many articles and exclude duplicates.\nconn \u0026lt;- dbConnect(RSQLite::SQLite(), \u0026quot;:memory:\u0026quot;) dbWriteTable(conn,\u0026quot;Title\u0026quot;,kw_clean) final \u0026lt;- keyword for (i in 1:nrow(keyword)){ each \u0026lt;- keyword[i,] dbWriteTable(conn, \u0026quot;aa\u0026quot;, each, append = TRUE) final[i,'paper_count'] \u0026lt;- dbGetQuery(conn, \u0026quot;SELECT count (distinct title) from Title where word in (select word from aa)\u0026quot;) } dbDisconnect(conn) Thirdly, generate the keyword coverage plot.\na \u0026lt;- ggplot(final) + geom_point(aes(x=keyword_count, y=paper_count)) + geom_label( label=\u0026quot;(200,10685)\u0026quot;, x=200, y=10800, label.padding = unit(0.55, \u0026quot;lines\u0026quot;), label.size = 0.30, vjust = 0, ) + labs( x = \u0026quot;keyword count\u0026quot;, y = \u0026quot;paper count\u0026quot;, title = \u0026quot;Keyword Coverage\u0026quot;) + theme_classic(base_size = 12) ggplotly(a) We can see that as the keywords increase, the more articles are covered. However, when the keywords increase to a certain range, the coverage rate becomes limited.\nThe results show that the most frequent 50 keywords can cover 87% (9826/11222) of the articles. The top 100 can cover 92% (10365/11222) of the articles and the top 200 can cover more than 95% (10685/11222) of the articles.\nTokenization by n-gram We‚Äôve been using the unnest_tokens function to tokenize by word, but we can also use the function to tokenize into consecutive sequences of words, called n-grams. By seeing how often word X is followed by word Y, we can then build a model of the relationships between them.\n# tokenizing by n-gram bigram \u0026lt;- data2 %\u0026gt;% unnest_tokens(bigram, keyword, token = \u0026quot;ngrams\u0026quot;, n = 2) %\u0026gt;% distinct() Now we use tidyr‚Äôs separate(), which splits a column into multiple columns based on a delimiter. This lets us separate it into two columns, ‚Äúword1‚Äù and ‚Äúword2‚Äù, at which point we can remove cases where either is a stop-word.\n# separate words bigram_separated \u0026lt;- bigram %\u0026gt;% separate(bigram, c(\u0026quot;word1\u0026quot;, \u0026quot;word2\u0026quot;), sep = \u0026quot; \u0026quot;) %\u0026gt;% # lemmatizing words mutate(word1 = case_when(length(word1) \u0026lt; 6 | word1 %in% c('ods','data','mining','learning') ~ word1, TRUE ~ lemmatize_words(word1)), word2 = case_when(length(word2) \u0026lt; 6 | word2 %in% c('ods','data','mining','learning') ~ word2, TRUE ~ lemmatize_words(word2))) # filter stop words and NA stopword \u0026lt;- stopwords::stopwords(\u0026quot;en\u0026quot;) bigram_filtered \u0026lt;- bigram_separated %\u0026gt;% filter(!word1 %in% stopword \u0026amp; is.na(as.numeric(word1))) %\u0026gt;% filter(!word2 %in% stopword \u0026amp; is.na(as.numeric(word2))) %\u0026gt;% filter(word1 != word2) # new bigram counts bigram_count \u0026lt;- bigram_filtered %\u0026gt;% count(word1, word2, sort = TRUE) kable(bigram_count[1:10,]) word1 word2 n enterprise guide 380 proc report 375 sas macro 164 sas graph 157 data step 144 data management 116 proc sql 113 data warehouse 112 clinical trial 110 Network analysis We may be interested in visualizing all of the relationships among words simultaneously. As one common visualization, we can arrange the words into a network graph. A graph can be constructed from a tidy object since it has three variables:\nfrom: the node an edge is coming from to: the node an edge is going towards weight: a numeric value associated with each edge We use the graph_from_data_frame() function from the package igraph, which takes a data frame of edges with columns for ‚Äúfrom‚Äù, ‚Äúto‚Äù, and edge attributes (in this case n). Then we use the ggraph package to convert the igraph object into a ggraph with the ggraph() function.\n# filter for only relatively common combinations bigram_graph \u0026lt;- bigram_count %\u0026gt;% filter(n \u0026gt; 35) %\u0026gt;% graph_from_data_frame() # network graph set.seed(999) a \u0026lt;- grid::arrow(type = \u0026quot;closed\u0026quot;, length = unit(.15, \u0026quot;inches\u0026quot;)) ggraph(bigram_graph, layout = \u0026quot;fr\u0026quot;) + geom_edge_link(aes(edge_alpha = n), show.legend = FALSE, arrow = a, end_cap = circle(.07, 'inches')) + geom_node_point(color = \u0026quot;lightblue\u0026quot;, size = 5) + geom_node_text(aes(label = name), vjust = 1, hjust = 1) + theme_void() "},{"uri":"https://rc-web-crawler.github.io/2nd_home/6_content/","title":"Content Analysis","tags":[],"description":"","content":"Title Analysis: Keyword Prediction Although most of the articles have keywords with them, there are still quite a lot of articles without keywords. Meanwhile, some articles may have complicated keywords, which could also increase the difficulty of analysis. To solve this problem, we can try to predict the keywords with title analysis techniques.\nIn the following parts, the keyword prediction using the results from previous keyword analysis will be demonstrated.\nPre-prepare: Include packages and source data We will mainly use the package quanteda for our analysis. Quanteda is an R package for managing and analyzing textual data. It is designed for R users needing to apply natural language processing to texts, from documents to final analysis.\npackage loading library(tidyverse) library(tidytext) library(dplyr) library(XML) library(knitr) library(tm) library(corpus) library(quanteda) library(readxl) library(topicmodels) library(textstem) rm(list = ls()) set.seed(1234) #Load data paper \u0026lt;- read_excel(\u0026quot;~/tm/app-1/new_kw.xlsx\u0026quot;) Pre-prepare: Including data and building dictonary In our analysis, we will use topic-specific dictionaries. Topic-specific dictionaries is a method somehow similar to sentiment analysis. Its aim is to determine the polarity of a text, which could be done by counting terms that were previously assigned to the given categories. With this method, we can categorize the given titles into specific keywords in our dictionary.\nTo use topic-specific dictionaries, the keyword corpus and dictionary need to be built in advance.\nbuild dictionary #Build keyword corpus paper2 \u0026lt;- paper %\u0026gt;% select(title, keyword) %\u0026gt;% mutate(keyword2 = lemmatize_strings(str_to_lower(keyword))) corp_k \u0026lt;- corpus(paper2, text_field = 'keyword2') token_k \u0026lt;- quanteda::tokens(corp_k, remove_numbers = TRUE, remove_punct = TRUE, remove_symbols = TRUE) #Build dictionary based on keyword analysis result dict \u0026lt;- list( output_delivery_system = c('output','delivery','system'), hash_object = c('hash','object'), machine_learing = c('machine','learing'), dictionary_table = c('dictionary','table'), cdisc = c('cdisc','adam','sdtm','xml','domain','send'), multisheet_workbook = c('multi','sheet','workbook','excel'), business_intelligence = c('business','intelligence'), call_execute = c('call','execute'), repeated_measures = c('repeat','measures'), tagset = c('excelxp', 'tageset'), logistic_regression = c('logistic', 'regression'), time_series = c('time','series'), sas_af = c('sas','af'), sas_viya = c('sas','viya'), sas_internet = c('sas','internet'), sas_base = c('sas','base'), sas_graph = c('sas', 'visual', 'analytics','graph'), sas_consult = c('sas', 'consult', 'consultant'), sas_enterprise = c('sas', 'enterprise','guide','miner'), ods = c('ods','rtf','graphics','microsoft','od'), macro = c('macro'), array = c('array'), healthcare = c('healthcare'), format = c('format'), html = c('html'), global_forum = c('global','forum'), proc_sql = c('proc','sql'), proc_report = c('proc','report'), proc_template = c('proc','template','sasgf'), proc_tabulate = c('proc','tabulate'), clinical_trial = c('clinical','trial'), survival_analysis = c('survival','analysis'), data_warehouse = c('datum','warehouse'), data_mining = c('datum','mining'), data_management = c('datum','management'), data_integration = c('datum','integration'), data_quality = c('datum','quality','clean'), data_visualization = c('datum','visualization'), project_management = c('project','management') ) lexicon \u0026lt;- dictionary(dict) Keyword Cleaning We will calculate the keyword coverage from source data first. In this study, the keyword coverage is defined as the percent of papers with any keyword existing after imputation. Keyword cleaning is performed before the calculation.\nkeyword cleaning #Clean Keyword dfm_k \u0026lt;- dfm(token_k) %\u0026gt;% dfm_remove(c(stopwords(\u0026quot;english\u0026quot;),'√¢','sas','datum')) %\u0026gt;% dfm_group(groups = docvars(token_k)[,\u0026quot;title\u0026quot;]) %\u0026gt;% dfm_lookup(dictionary = lexicon) dfm.prop.k \u0026lt;- dfm_weight(dfm_k, scheme = \u0026quot;prop\u0026quot;) df.prop.k \u0026lt;- convert(dfm.prop.k, \u0026quot;data.frame\u0026quot;) ncol_k \u0026lt;- ncol(df.prop.k) for (i in 1:nrow(df.prop.k)){ df.prop.k[i,'max'] \u0026lt;- max(df.prop.k[i,c(seq(2, ncol_k))]) df.prop.k[i,'keyword_cleaned'] \u0026lt;- '' for (j in 2:ncol_k){ if (df.prop.k[i, j] == df.prop.k[i,'max'] \u0026amp; df.prop.k[i,'max'] != 0){ df.prop.k[i,'keyword_cleaned'] \u0026lt;- paste(df.prop.k[i,'keyword_cleaned'], colnames(df.prop.k)[j]) } } } #cleaned keyword coverage paper3 \u0026lt;- paper2 %\u0026gt;% inner_join(df.prop.k, by=c('title' = 'doc_id')) %\u0026gt;% select(title, keyword2, keyword_cleaned) %\u0026gt;% mutate(null = ifelse(keyword_cleaned == '' \u0026amp; grepl('kb', keyword2), 'Y', 'N'), coverage = ifelse(keyword_cleaned == \u0026quot;\u0026quot;, \u0026quot;N\u0026quot;, \u0026quot;Y\u0026quot;)) %\u0026gt;% filter(null == 'N') fail \u0026lt;- paper3 %\u0026gt;% filter(coverage == 'N') %\u0026gt;% group_by(keyword2) %\u0026gt;% summarise(n = n()) %\u0026gt;% arrange(desc(n)) cov_clean \u0026lt;- paper3 %\u0026gt;% group_by(coverage) %\u0026gt;% summarise(n = n()) %\u0026gt;% ungroup() %\u0026gt;% mutate(percentage = round(n/sum(n)*100,2)) %\u0026gt;% mutate(lab1=paste(coverage , n,sep=':'))#Coverage:73.18% of 13072 ggplot(cov_clean, aes(x='',y=n,fill=lab1)) + geom_bar (stat=\u0026quot;identity\u0026quot;, width=1) + coord_polar (\u0026quot;y\u0026quot;, start=0) + geom_text(aes(label = paste0(percentage, \u0026quot;%\u0026quot;)), position = position_stack(vjust=0.5))+ labs(x = NULL, y = NULL, fill = NULL,title=\u0026quot;Coverage of cleaned keywords\u0026quot;) + theme_classic() + theme(axis.line = element_blank(), axis.text = element_blank(), axis.ticks = element_blank() ) + scale_fill_brewer(palette=\u0026quot;Blues\u0026quot;) Approximately 73.18% of the papers have cleaned keyword coverage.\nKeyword Imputation Next, we will use our dictionary to impute the keyword from the titles. Similarly, the coverage of the imputed keyword will be calculated. Also, we will calculate the accuracy of the imputation by comparing the imputed results with cleaning results. The accuracy here is defined as the percentage of papers where the cleaned keywords exist in the imputed keywords for all the papers imputed.\nkeyword imputation #Build title corpus paper4 \u0026lt;- paper3 %\u0026gt;% filter(coverage == 'Y') %\u0026gt;% mutate(title2 = lemmatize_strings(str_to_lower(title))) corp_t \u0026lt;- corpus(paper4, text_field = 'title2') token_t \u0026lt;- quanteda::tokens(corp_t, remove_numbers = TRUE, remove_punct = TRUE, remove_symbols = TRUE) #Impute Keyword dfm_t \u0026lt;- dfm(token_t) %\u0026gt;% dfm_remove(c(stopwords(\u0026quot;english\u0026quot;),'√¢','sas','datum')) %\u0026gt;% dfm_group(groups = docvars(token_t)[,\u0026quot;title\u0026quot;]) %\u0026gt;% dfm_lookup(dictionary = lexicon) dfm.prop.t \u0026lt;- dfm_weight(dfm_t, scheme = \u0026quot;prop\u0026quot;) df.prop.t \u0026lt;- convert(dfm.prop.t, \u0026quot;data.frame\u0026quot;) ncol_t \u0026lt;- ncol(df.prop.t) for (i in 1:nrow(df.prop.t)){ df.prop.t[i,'max'] \u0026lt;- max(df.prop.t[i,c(seq(2, ncol_t))]) df.prop.t[i,'keyword_imputed'] \u0026lt;- '' for (j in 2:ncol_t){ if (df.prop.t[i, j] == df.prop.t[i,'max'] \u0026amp; df.prop.t[i,'max'] != 0){ df.prop.t[i,'keyword_imputed'] \u0026lt;- paste(df.prop.t[i,'keyword_imputed'], colnames(df.prop.t)[j]) } } } #imputed keyword coverage paper5 \u0026lt;- paper4 %\u0026gt;% inner_join(df.prop.t, by=c('title' = 'doc_id')) %\u0026gt;% select(title, keyword_cleaned, keyword_imputed) %\u0026gt;% mutate(coverage = ifelse(keyword_imputed == \u0026quot;\u0026quot;, \u0026quot;N\u0026quot;, \u0026quot;Y\u0026quot;)) cov_imp \u0026lt;- paper5 %\u0026gt;% group_by(coverage) %\u0026gt;% summarise(n = n()) %\u0026gt;% ungroup() %\u0026gt;% mutate(percentage = round(n/sum(n)*100,2)) %\u0026gt;% mutate(lab1=paste(coverage , n,sep=':'))#Coverage:73.18% of 13072 ggplot(cov_imp, aes(x='',y=n,fill=lab1)) + geom_bar (stat=\u0026quot;identity\u0026quot;, width=1) + coord_polar (\u0026quot;y\u0026quot;, start=0) + geom_text(aes(label = paste0(percentage, \u0026quot;%\u0026quot;)), position = position_stack(vjust=0.5))+ labs(x = NULL, y = NULL, fill = NULL, title=\u0026quot;Coverage of imputed keywords\u0026quot;) + theme_classic() + theme(axis.line = element_blank(), axis.text = element_blank(), axis.ticks = element_blank() ) + scale_fill_brewer(palette=\u0026quot;Blues\u0026quot;) #imputed keyword accuracy test \u0026lt;- paper5 %\u0026gt;% filter(coverage == 'Y') %\u0026gt;% mutate(result = keyword_cleaned %in% keyword_imputed, success = sum(result)) cov_acc \u0026lt;- test %\u0026gt;% group_by(coverage) %\u0026gt;% summarise(n = n()) %\u0026gt;% ungroup() %\u0026gt;% mutate(percentage = round(n/sum(n)*100,2)) %\u0026gt;% mutate(lab1=paste(coverage , n,sep=':')) ggplot(cov_acc, aes(x='',y=n,fill=lab1)) + geom_bar (stat=\u0026quot;identity\u0026quot;, width=1) + coord_polar (\u0026quot;y\u0026quot;, start=0) + geom_text(aes(label = paste0(percentage, \u0026quot;%\u0026quot;)), position = position_stack(vjust=0.5))+ labs(x = NULL, y = NULL, fill = NULL,title=\u0026quot;Percent of imputed keywords that exist in cleaned keywords\u0026quot;) + theme_classic() + theme(axis.line = element_blank(), axis.text = element_blank(), axis.ticks = element_blank() ) + scale_fill_manual(values='#71A92C') Approximately 82.1% of the papers have keyword coverage after imputation. Also, we can find that all the imputed keywords are within cleaned keywords.\nKeyword Prediction Finally, we try to use the dictionary to predict the keyword of all the papers we collected.\nkeyword prediction #Prediction paper_all \u0026lt;- read_excel(\u0026quot;paper.xlsx\u0026quot;) paper_pred \u0026lt;- paper_all %\u0026gt;% anti_join(paper5, by = 'title') %\u0026gt;% mutate(title2 = lemmatize_strings(str_to_lower(title))) corp_p \u0026lt;- corpus(paper_pred, text_field = 'title2') token_p \u0026lt;- quanteda::tokens(corp_p, remove_numbers = TRUE, remove_punct = TRUE, remove_symbols = TRUE) #Predict Keyword dfm_p \u0026lt;- dfm(token_p) %\u0026gt;% dfm_remove(c(stopwords(\u0026quot;english\u0026quot;),'√¢','sas','datum')) %\u0026gt;% dfm_group(groups = docvars(token_p)[,\u0026quot;title\u0026quot;]) %\u0026gt;% dfm_lookup(dictionary = lexicon) dfm.prop.p \u0026lt;- dfm_weight(dfm_p, scheme = \u0026quot;prop\u0026quot;) df.prop.p \u0026lt;- convert(dfm.prop.p, \u0026quot;data.frame\u0026quot;) ncol_p \u0026lt;- ncol(df.prop.p) for (i in 1:nrow(df.prop.p)){ df.prop.p[i,'max'] \u0026lt;- max(df.prop.p[i,c(seq(2, ncol_p))]) df.prop.p[i,'keyword_predict'] \u0026lt;- '' for (j in 2:ncol_p){ if (df.prop.p[i, j] == df.prop.p[i,'max'] \u0026amp; df.prop.p[i,'max'] != 0){ df.prop.p[i,'keyword_predict'] \u0026lt;- paste(df.prop.p[i,'keyword_predict'], colnames(df.prop.p)[j]) } } } #predict keyword coverage paper_pred \u0026lt;- paper_pred %\u0026gt;% inner_join(df.prop.p, by=c('title' = 'doc_id')) %\u0026gt;% mutate(coverage = ifelse(keyword_predict == \u0026quot;\u0026quot;, \u0026quot;N\u0026quot;, \u0026quot;Y\u0026quot;)) cov_pred \u0026lt;- paper_pred %\u0026gt;% group_by(coverage) %\u0026gt;% summarise(n = n()) %\u0026gt;% ungroup() %\u0026gt;% mutate(percentage = round(n/sum(n)*100,2)) %\u0026gt;% mutate(lab1=paste(coverage , n,sep=':'))#Coverage:73.18% of 13072 ggplot(cov_pred, aes(x='',y=n,fill=lab1)) + geom_bar (stat=\u0026quot;identity\u0026quot;, width=1) + coord_polar (\u0026quot;y\u0026quot;, start=0) + geom_text(aes(label = paste0(percentage, \u0026quot;%\u0026quot;)), position = position_stack(vjust=0.5))+ labs(x = NULL, y = NULL, fill = NULL,title=\u0026quot;Coverage of predicted keywords for all papers\u0026quot;) + theme_classic() + theme(axis.line = element_blank(), axis.text = element_blank(), axis.ticks = element_blank() ) + scale_fill_brewer(palette=\u0026quot;Blues\u0026quot;) After predicting keywords using our dictionary model, the predicted keyword coverage of all the papers is approximately 53.16%.\nFurther Explorations and conclusion Our exploration indicates that dictionary model could be useful when keywords are not presented. After prediction with the model, the keywords of more than half of the papers could be imputed. Moreover, the dictionary used in our study is relatively crude. With a better dictionary, the coverage could be even higher.\nBesides, we also tried to use Latent Dirichlet Allocation(LDA) to perform a topic modeling analysis. However, because the number of words in a title is relatively small, we did not get a satisfying result.\nOther methods may also be applied in the topic analysis, such as supervised machine learning. If you are interested, have a try!\n"},{"uri":"https://rc-web-crawler.github.io/2nd_home/7_shiny/","title":"eCRF Index Shiny App","tags":[],"description":"","content":"Instruction Users could upload the eCRF pdf file using new template to get the index for eCRF forms and map with SDTM domains. According to alpha test results, the coverage rate is 70% ~ 85% and the accuracy rate is 85% ~ 100%.\nLimitation As expected, mapping results for efficacy-related domains, like QS, ZW, are low in accuracy and coverage, since their eCRF forms are always study specific forms\nShiny App "},{"uri":"https://rc-web-crawler.github.io/","title":"","tags":[],"description":"","content":"Refresh Corner Welcome to our Refresh Corner study group!üòé ~ ü§πüèª‚Äç‚ôÇÔ∏è Enjoy your journey ~ "},{"uri":"https://rc-web-crawler.github.io/categories/","title":"Categories","tags":[],"description":"","content":""},{"uri":"https://rc-web-crawler.github.io/credits/","title":"credits","tags":[],"description":"","content":"References Beautiful soup4 documentation: https://www.crummy.com/software/BeautifulSoup/bs4/doc/ Lexjansen Paper website: https://www.lexjansen.com XPATH cheat sheet: https://devhints.io/xpath CSS selector cheat sheet: https://frontend30.com/css-selectors-cheatsheet/ "},{"uri":"https://rc-web-crawler.github.io/tags/","title":"Tags","tags":[],"description":"","content":""}]