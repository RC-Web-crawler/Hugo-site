[{"uri":"https://rc-web-crawler.github.io/1st_home/","title":"Web-Crawler","tags":[],"description":"","content":"Refresh Corner: Web-Crawler This is the webpage for our Web-Crawler study groupüòÑ Team members: Aiwenüëßüèª, Haoyongüßëüèª, Mengyueüë©üèª, Shuyanüë©üèª‚Äçüíº, Songrenüë©üèª‚Äçüî¨ Intro to Web Scraping 1.1 Web at a glance 1.2 HTML elements \u0026amp; Tree Structure 1.3 CSS selectors \u0026amp; Xpath 1.4 Demo: Web Scraping with R/Python 1.5 Appendix: Python Environment Setup on work laptop Video Intro_to_webcrawler.mp4 (9 MB) "},{"uri":"https://rc-web-crawler.github.io/1st_home/1_web_at_glance/","title":"1.1 Web at a glance","tags":[],"description":"","content":" 1. Web at a glance 1.1 Overview of a webpage 1.1.1 HTML 1.1.2 CSS 1.1.3 JavaScript Is web crawling and scraping legal?\nOctoparse : If you‚Äôre doing web crawling for your own purposes, then it is legal as it falls under fair use doctrine. The complications start if you want to use scraped data for others, especially commercial purposes. As long as you are not crawling at a disruptive rate and the source is public you should be fine. Quora : You can crawl any page you like, scraping in itself is not illegal. The worst case scenario would be if you got blocked by the website if you do not follow the rules stated in the robots.txt.\nWeb crawling Vs. Web scraping - Web crawling, also known as Indexing is used to index the information on the page using bots also known as crawlers.\n- Web Crawlers are basically used by major search engines like Google, Bing, Yahoo, statistical agencies, and large online aggregators.\n- When a bot crawls a website, it goes through every page and every link, until the last line of the website, looking for ANY information.\n- The web crawling process usually captures generic information. üÜö - Web scraping, also known as web data extraction, is similar to web crawling in that it identifies and locates the target data from web pages.\n- The key difference, is that with web scraping, we know the exact data set identifier e.g. an HTML element structure for web pages that are being fixed, from which data needs to be extracted.\n- Web scraping is an automated way of extracting specific data sets using bots which are also known as ‚Äòscrapers‚Äô. What is a robots.txt file? Robots.txt is a text file webmasters create to instruct web robots (typically search engine robots) how to crawl pages on their website. The robots.txt file is part of the the robots exclusion protocol (REP), a group of web standards that regulate how robots crawl the web, access and index content, and serve that content up to users.\n1. Web at a glance What is the Web?\nThe World Wide Web (WWW), commonly known as the Web, is an information system. It is an amazing collection of interconnected documents.\nWebpage/Website is a metaphor applied to the web.\nThe purpose of a Web browser (Chrome, Edge, Firefox, Safari) is to read HTML documents and display them correctly. A browser does not display the HTML tags, but uses them to determine how to display the document\n1.1 Overview of a webpage A webpage generally consists of three parts, namely HTML(Ë∂ÖÊñáÊú¨Ê†áËÆ∞ËØ≠Ë®Ä), CSS(Â±ÇÂè†Ê†∑ÂºèË°®) and JavaScript(Ê¥ªÂä®ËÑöÊú¨ËØ≠Ë®Ä).\nHTML provides the basic structure of sites, which is enhanced and modified by other technologies like CSS and JavaScript. CSS is used to control presentation, formatting, and layout. JavaScript is used to control the behavior of different elements. Â¶ÇÊûúÁî®‰∫∫‰ΩìÊù•ÊØîÂñªÔºåHTML ÊòØ‰∫∫ÁöÑÈ™®Êû∂ÔºåÂπ∂‰∏îÂÆö‰πâ‰∫Ü‰∫∫ÁöÑÂò¥Â∑¥„ÄÅÁúºÁùõ„ÄÅËÄ≥ÊúµÁ≠âË¶ÅÈïøÂú®Âì™Èáå„ÄÇCSS ÊòØ‰∫∫ÁöÑÂ§ñËßÇÁªÜËäÇÔºåÂ¶ÇÂò¥Â∑¥Èïø‰ªÄ‰πàÊ†∑Â≠êÔºåÁúºÁùõÊòØÂèåÁúºÁöÆËøòÊòØÂçïÁúºÁöÆÔºåÊòØÂ§ßÁúºÁùõËøòÊòØÂ∞èÁúºÁùõÔºåÁöÆËÇ§ÊòØÈªëËâ≤ÁöÑËøòÊòØÁôΩËâ≤ÁöÑÁ≠â„ÄÇJavaScript Ë°®Á§∫‰∫∫ÁöÑÊäÄËÉΩÔºå‰æãÂ¶ÇË∑≥Ëàû„ÄÅÂî±Ê≠åÊàñËÄÖÊºîÂ•è‰πêÂô®Á≠â„ÄÇ\n1.1.1 HTML HTML stands for HyperText Markup Language. \u0026ldquo;Markup language\u0026rdquo; means that, rather than using a programming language to perform functions, HTML uses tags to identify different types of content and the purposes they each serve to the webpage.\nLet\u0026rsquo;s start with a simple webpage without CSS and JavaScriptÔºöwebpage_raw\nHTML Document:\n\u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta charset=\u0026#34;utf-8\u0026#34;\u0026gt; \u0026lt;title\u0026gt;Áà¨Ëô´Â∞èÁªÑÁöÑÊµãËØïÈ°µ\u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;h1\u0026gt;Ê¨¢ËøéÊù•Âà∞Áà¨Ëô´Â∞èÁªÑÁöÑÁΩëÈ°µÔºÅ\u0026lt;/h1\u0026gt; \u0026lt;p\u0026gt;ËøôÊòØ‰∏Ä‰∏™ÁÆÄÂçïÁöÑÊµãËØïÈ°µ\u0026lt;/p\u0026gt; \u0026lt;h2\u0026gt;Âõ¢ÈòüÊàêÂëò‰∏éÈáçË¶ÅÊó•Êúü\u0026lt;/h2\u0026gt; \u0026lt;p\u0026gt;\u0026lt;b\u0026gt;2020Âπ¥6Êúà10Êó•\u0026lt;/b\u0026gt; ‚Äî ‰∫éÊòäÊ∞∏Âä†ÂÖ•ËØ∫ÂçéÁöÑÊó•Â≠ê\u0026lt;/p\u0026gt; \u0026lt;p\u0026gt;\u0026lt;b\u0026gt;2020Âπ¥8Êúà20Êó•\u0026lt;/b\u0026gt; ‚Äî Êù®Â∏ÜÂä†ÂÖ•ËØ∫ÂçéÁöÑÊó•Â≠ê\u0026lt;/p\u0026gt; \u0026lt;p\u0026gt;\u0026lt;b\u0026gt;2021Âπ¥6Êúà1Êó•\u0026lt;/b\u0026gt; ‚Äî ÁéãÂ¥ß‰∫∫Âä†ÂÖ•ËØ∫ÂçéÁöÑÊó•Â≠ê\u0026lt;/p\u0026gt; \u0026lt;p\u0026gt;\u0026lt;b\u0026gt;2021Âπ¥7Êúà1Êó•\u0026lt;/b\u0026gt; ‚Äî Èü©Ê¢¶Â≤≥Âä†ÂÖ•ËØ∫ÂçéÁöÑÊó•Â≠ê\u0026lt;/p\u0026gt; \u0026lt;p\u0026gt;\u0026lt;b\u0026gt;2021Âπ¥7Êúà1Êó•\u0026lt;/b\u0026gt; ‚Äî Êªï‰π¶Ë®ÄÂä†ÂÖ•ËØ∫ÂçéÁöÑÊó•Â≠ê\u0026lt;/p\u0026gt; \u0026lt;p\u0026gt;\u0026lt;b\u0026gt;2021Âπ¥7Êúà1Êó•\u0026lt;/b\u0026gt; ‚Äî ÂßöËâæÊñáÂä†ÂÖ•ËØ∫ÂçéÁöÑÊó•Â≠ê\u0026lt;/p\u0026gt; \u0026lt;h2\u0026gt;\u0026lt;img id=\u0026#34;myImage\u0026#34; src=\u0026#34;https://www.w3schools.com/js/pic_bulboff.gif\u0026#34; style=\u0026#34;width:30px\u0026#34;\u0026gt;Êõ¥Â§ö‰ø°ÊÅØ \u0026lt;/h2\u0026gt; \u0026lt;p\u0026gt;Gitlab: \u0026lt;a href=\u0026#34;http://gitlabce.apps.dit-prdocp.novartis.net/YUHAY/web-crawler-do.git\u0026#34;\u0026gt;Web Crawler DO\u0026lt;/a\u0026gt; \u0026lt;/p\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; You can see a lot of tags in this document, such as \u0026lt;html\u0026gt;, \u0026lt;/html\u0026gt;, \u0026lt;head\u0026gt;, \u0026lt;body\u0026gt;, \u0026lt;img\u0026gt;, etc.\n1.1.2 CSS CSS stands for Cascading Style Sheets. This programming language dictates how the HTML elements of a website should actually appear on the frontend of the page.\nLet\u0026rsquo;s see the new webpage after adding a CSS style: webpage_css\n\u0026lt;style type=\u0026#34;text/css\u0026#34;\u0026gt; @import url(\u0026#39;https://fonts.googleapis.com/css2?family=ZCOOL+KuaiLe\u0026amp;display=swap\u0026#39;); body {background-image: url(\u0026#39;https://www.statnews.com/wp-content/uploads/2018/05/Novartis-1.jpg\u0026#39;); font-family: \u0026#39;ZCOOL KuaiLe\u0026#39;;} h1 {color: blanchedalmond;font-size: 50px;} p {color:tomato; font-size: 25px;} h2 {color: yellow;font-size: 30px;} a {color:turquoise;} \u0026lt;/style\u0026gt; 1.1.3 JavaScript JavaScript is a logic-based programming language that can be used to modify website content and make it behave in different ways in response to a user\u0026rsquo;s actions. Most of the dynamic behavior you\u0026rsquo;ll see on a web page is thanks to JavaScript, which augments a browser\u0026rsquo;s default controls and behaviors.\nLet\u0026rsquo;s see the new webpage after applying a JavaScriptÔºöwebpage_js\n"},{"uri":"https://rc-web-crawler.github.io/1st_home/2_html_elements/","title":"HTML elements &amp; Tree Structure","tags":[],"description":"","content":" 2. HTML elements \u0026amp; Tree Structure 2.1 HTML elements 2.2 HTML Attributes 2.3 Tree structure of HTML document 2.3.1 Relationship of Nodes 2. HTML elements \u0026amp; Tree Structure 2.1 HTML elements An HTML element is defined by a start tag, some content, and an end tag. The HTML element is everything from the start tag to the end tag:\n\u0026lt;tagname\u0026gt;Content goes here...\u0026lt;/tagname\u0026gt;\nExamples:\n\u0026lt;h1\u0026gt;Ê¨¢ËøéÊù•Âà∞Áà¨Ëô´Â∞èÁªÑÁöÑÁΩëÈ°µÔºÅ\u0026lt;/h1\u0026gt;\n\u0026lt;p\u0026gt;ËøôÊòØ‰∏Ä‰∏™ÁÆÄÂçïÁöÑÊµãËØïÈ°µ \u0026lt;/p\u0026gt;\nStart tag Element content End tag \u0026lt;h1\u0026gt; Ê¨¢ËøéÊù•Âà∞Áà¨Ëô´Â∞èÁªÑÁöÑÁΩëÈ°µ \u0026lt;/h1\u0026gt; \u0026lt;p\u0026gt; ËøôÊòØ‰∏Ä‰∏™ÁÆÄÂçïÁöÑÊµãËØïÈ°µ \u0026lt;p\u0026gt; \u0026lt;br\u0026gt; none none Note: Some HTML elements have no content (like the \u0026lt;br\u0026gt; element). These elements are called empty elements. Empty elements do not have an end tag!\nCommon elements explained:\nThe \u0026lt;!DOCTYPE html\u0026gt; declaration defines that this document is an HTML5 document.\nThe \u0026lt;html\u0026gt; element is the root element of an HTML page\nThe \u0026lt;head\u0026gt; element contains meta information about the HTML page\nThe \u0026lt;title\u0026gt; element specifies a title for the HTML page (which is shown in the browser\u0026rsquo;s title bar or in the page\u0026rsquo;s tab)\nThe \u0026lt;body\u0026gt; element defines the document\u0026rsquo;s body, and is a container for all the visible contents, such as headings, paragraphs, images, hyperlinks, tables, lists, etc.\nThe \u0026lt;h1\u0026gt;\u0026lt;h2\u0026gt; element defines a large heading\nThe \u0026lt;p\u0026gt; element defines a paragraph\n2.2 HTML Attributes HTML attributes provide additional information about HTML elements. All HTML elements can have attributes. Attributes are always specified in the start tag. Attributes usually come in name/value pairs like: name=\u0026ldquo;value\u0026rdquo;.\nThe href AttributeÔºö\nThe \u0026lt;a\u0026gt; tag defines a hyperlink. The href attribute specifies the URL of the page the link goes to. Example:\n\u0026lt;a href=\u0026quot;http://gitlabce.apps.dit-prdocp.novartis.net/YUHAY/web-crawler-do.git\u0026quot;\u0026gt;Web Crawler DO\u0026lt;/a\u0026gt;\nThe src AttributeÔºö\nThe \u0026lt;img\u0026gt; tag is used to embed an image in an HTML page. The src attribute specifies the path to the image to be displayed. Example:\n\u0026lt;img src=\u0026quot;https://www.w3schools.com/js/pic_bulboff.gif\u0026quot;\u0026gt;\nThe style AttributeÔºö\nThe style attribute is used to add styles to an element, such as color, font, size, and more. Example:\n\u0026lt;img style=\u0026quot;width:30px\u0026quot;\u0026gt;\nThe width and height attributes of \u0026lt;img\u0026gt; provide size information for images\nThe alt attribute of \u0026lt;img\u0026gt; provides an alternate text for an image\nThe lang attribute of the \u0026lt;html\u0026gt; tag declares the language of the Web page\nThe title attribute defines some extra information about an element\n2.3 Tree structure of HTML document HTML documents can be treated as trees of nodes. Look at the following document:\n\u0026lt;books\u0026gt; \u0026lt;book\u0026gt; \u0026lt;title lang=\u0026#34;en\u0026#34;\u0026gt;Harry Potter\u0026lt;/title\u0026gt; \u0026lt;author\u0026gt;J K. Rowling\u0026lt;/author\u0026gt; \u0026lt;year\u0026gt;2005\u0026lt;/year\u0026gt; \u0026lt;price\u0026gt;29.99\u0026lt;/price\u0026gt; \u0026lt;/book\u0026gt; \u0026lt;/books\u0026gt; The topmost element of the tree is called the root element. \u0026lt;books\u0026gt; is the root element node of the above tree. There are other element nodes, such as \u0026lt;author\u0026gt;J K. Rowling\u0026lt;/author\u0026gt;, \u0026lt;year\u0026gt;2005\u0026lt;/year\u0026gt;, etc.\nIt also looks like the path of computer file systems:\n2.3.1 Relationship of Nodes Example 1:\n\u0026lt;book\u0026gt; \u0026lt;title\u0026gt;Harry Potter\u0026lt;/title\u0026gt; \u0026lt;author\u0026gt;J K. Rowling\u0026lt;/author\u0026gt; \u0026lt;year\u0026gt;2005\u0026lt;/year\u0026gt; \u0026lt;price\u0026gt;29.99\u0026lt;/price\u0026gt; \u0026lt;/book\u0026gt; 1. Parent\nEach element has one parent.\nIn the example 1; the book element is the parent of the title, author, year, and price.\n2. Children\nElement nodes may have zero, one or more children.\nIn the example 1; the title, author, year, and price elements are all children of the book element.\n3. Siblings\nNodes that have the same parent.\nIn the example 1; the title, author, year, and price elements are all siblings.\nExample 2:\n\u0026lt;books\u0026gt; \u0026lt;book\u0026gt; \u0026lt;title\u0026gt;Harry Potter\u0026lt;/title\u0026gt; \u0026lt;author\u0026gt;J K. Rowling\u0026lt;/author\u0026gt; \u0026lt;year\u0026gt;2005\u0026lt;/year\u0026gt; \u0026lt;price\u0026gt;29.99\u0026lt;/price\u0026gt; \u0026lt;/book\u0026gt; \u0026lt;/books\u0026gt; 4. Ancestors\nA node\u0026rsquo;s parent, parent\u0026rsquo;s parent, etc.\nIn the example 2; the ancestors of the title element are the book element and the books element.\n5. Descendants\nA node\u0026rsquo;s children, children\u0026rsquo;s children, etc.\nIn the example 2; descendants of the books element are the book, title, author, year, and price elements.\n"},{"uri":"https://rc-web-crawler.github.io/1st_home/3_css/","title":"CSS selector &amp; Xpath","tags":[],"description":"","content":"3.1 Introduction CSS, Cascading Style Sheets, is a style sheet language used for describing the presentation of a document written in a markup language such as HTML. And CSS selectors are used to select the content you want to style. Selectors are the part of CSS rule set. CSS selectors select HTML elements according to its id, class, type, attribute etc.\nXPath, XML Path Language, is a query language for selecting nodes from an XML document. In addition, XPath may be used to compute values from the content of an XML document. XPath was defined by the World Wide Web Consortium. CSS Selector vs XPATH\nNo. HTML XML 1) HTML is used¬†to display data¬†and focuses on how data looks. XML is a software and hardware independent tool used¬†to transport and store data. It focuses on what data is. 2) HTML is a¬†markup language¬†itself. XML provides a¬†framework to define markup languages. 3) HTML is¬†not case sensitive. XML is¬†case sensitive. 4) HTML is a presentation language. XML is neither a presentation language nor a programming language. 5) HTML¬†has its own predefined tags. You¬†can define tags according to your need. 6) In HTML, it is¬†not necessary to use a closing tag. XML¬†makes it mandatory to use a closing tag. 7) HTML is¬†static¬†because it is used to display data. XML is¬†dynamic¬†because it is used to transport data. 8) HTML¬†does not preserve whitespaces. XML¬†preserve whitespaces. HTML vs XML\n3.3 Examples Sample HTML: link\nExample 1:Abusolte path css selector xpath check_element_text(html = html, css = \u0026#34;body \u0026gt; div \u0026gt; p \u0026gt; t\u0026#34;) [1] \u0026ldquo;Beat COVID-19!\u0026rdquo; \u0026ldquo;Dumpling YYDS!\u0026rdquo; \u0026ldquo;Let\u0026rsquo;s romantic!\u0026rdquo; \u0026ldquo;Sweet Dumpling YYDS!\u0026rdquo;\ncheck_element_text(html = html, xpath = \u0026#34;/html/body/div/p/t\u0026#34;) [1] \u0026ldquo;Beat COVID-19!\u0026rdquo; \u0026ldquo;Dumpling YYDS!\u0026rdquo; \u0026ldquo;Let\u0026rsquo;s romantic!\u0026rdquo; \u0026ldquo;Sweet Dumpling YYDS!\u0026rdquo;\nExample 2:Relative path css selector xpath check_element_text(html = html, css = \u0026#34;body t\u0026#34;) [1] \u0026ldquo;Beat COVID-19!\u0026rdquo; \u0026ldquo;Dumpling YYDS!\u0026rdquo; \u0026ldquo;Let\u0026rsquo;s romantic!\u0026rdquo; \u0026ldquo;Sweet Dumpling YYDS!\u0026rdquo;\ncheck_element_text(html = html, xpath = \u0026#34;//body//t\u0026#34;) [1] \u0026ldquo;Beat COVID-19!\u0026rdquo; \u0026ldquo;Dumpling YYDS!\u0026rdquo; \u0026ldquo;Let\u0026rsquo;s romantic!\u0026rdquo; \u0026ldquo;Sweet Dumpling YYDS!\u0026rdquo;\nExample 3:Single element css selector xpath check_element_text(html = html, css = \u0026#34;t\u0026#34;) [1] \u0026ldquo;Beat COVID-19!\u0026rdquo; \u0026ldquo;Dumpling YYDS!\u0026rdquo; \u0026ldquo;Let\u0026rsquo;s romantic!\u0026rdquo; \u0026ldquo;Sweet Dumpling YYDS!\u0026rdquo;\ncheck_element_text(html = html, xpath = \u0026#34;//t\u0026#34;) [1] \u0026ldquo;Beat COVID-19!\u0026rdquo; \u0026ldquo;Dumpling YYDS!\u0026rdquo; \u0026ldquo;Let\u0026rsquo;s romantic!\u0026rdquo; \u0026ldquo;Sweet Dumpling YYDS!\u0026rdquo;\nExample 4:Multiple element css selector xpath check_element_text(html = html, css = \u0026#34;t, price\u0026#34;) [1] \u0026ldquo;Beat COVID-19!\u0026rdquo; \u0026ldquo;1000000000.00\u0026rdquo; [3] \u0026ldquo;Dumpling YYDS!\u0026rdquo; \u0026ldquo;100.00\u0026rdquo;\n[5] \u0026ldquo;Let\u0026rsquo;s romantic!\u0026rdquo; \u0026ldquo;1000.0\u0026rdquo; [7] \u0026ldquo;Sweet Dumpling YYDS!\u0026rdquo; \u0026ldquo;100.0\u0026rdquo;\ncheck_element_text(html = html, xpath = \u0026#34;//t | //price\u0026#34;) [1] \u0026ldquo;Beat COVID-19!\u0026rdquo; \u0026ldquo;1000000000.00\u0026rdquo; [3] \u0026ldquo;Dumpling YYDS!\u0026rdquo; \u0026ldquo;100.00\u0026rdquo;\n[5] \u0026ldquo;Let\u0026rsquo;s romantic!\u0026rdquo; \u0026ldquo;1000.0\u0026rdquo; [7] \u0026ldquo;Sweet Dumpling YYDS!\u0026rdquo; \u0026ldquo;100.0\u0026rdquo;\nExample 5:Position css selector xpath check_element_text(html = html, css = \u0026#34;div:first-of-type\u0026#34;) [1] \u0026ldquo;Welcome 2022! Beat COVID-19! 1000000000.00\u0026rdquo;\ncheck_element_text(html = html, css = \u0026#34;div:nth-of-type(2)\u0026#34;) [1] \u0026ldquo;Happy Spring Festival! Dumpling YYDS! 100.00\u0026rdquo;\ncheck_element_text(html = html, css = \u0026#34;div:last-of-type\u0026#34;) [1] \u0026ldquo;Happy Valentine\u0026rsquo;s Day! Let\u0026rsquo;s romantic! 1000.0 Happy Lantern Festival! Sweet Dumpling YYDS! 100.0\u0026rdquo;\ncheck_element_text(html = html, xpath = \u0026#34;//div[position()=1]\u0026#34;) [1] \u0026ldquo;Welcome 2022! Beat COVID-19! 1000000000.00\u0026rdquo;\ncheck_element_text(html = html, xpath = \u0026#34;//div[2]\u0026#34;) [1] \u0026ldquo;Happy Spring Festival! Dumpling YYDS! 100.00\u0026rdquo;\ncheck_element_text(html = html, xpath = \u0026#34;//div[last()]\u0026#34;) [1] \u0026ldquo;Happy Valentine\u0026rsquo;s Day! Let\u0026rsquo;s romantic! 1000.0 Happy Lantern Festival! Sweet Dumpling YYDS! 100.0\u0026rdquo;\nExample 6:ID attribute css selector xpath check_element_text(html = html, css = \u0026#34;div#div2\u0026#34;) [1] \u0026ldquo;Happy Spring Festival! Dumpling YYDS! 100.00\u0026rdquo;\ncheck_element_text(html = html, xpath = \u0026#34;//div[@id=\u0026#39;div2\u0026#39;]\u0026#34;) [1] \u0026ldquo;Happy Spring Festival! Dumpling YYDS! 100.00\u0026rdquo;\nExample 7:Class attribute css selector xpath check_element_text(html = html, css = \u0026#34;div.class1\u0026#34;) [1] \u0026ldquo;Welcome 2022! Beat COVID-19! 1000000000.00\u0026rdquo; [2] \u0026ldquo;Happy Valentine\u0026rsquo;s Day! Let\u0026rsquo;s romantic! 1000.0 Happy Lantern Festival! Sweet Dumpling YYDS! 100.0\u0026rdquo;\ncheck_element_text(html = html, css = \u0026#34;div.class1.class2\u0026#34;) [1] \u0026ldquo;Happy Valentine\u0026rsquo;s Day! Let\u0026rsquo;s romantic! 1000.0 Happy Lantern Festival! Sweet Dumpling YYDS! 100.0\u0026rdquo;\ncheck_element_text(html = html, xpath = \u0026#34;//div[@class=\u0026#39;class1\u0026#39;]\u0026#34;) [1] \u0026ldquo;Welcome 2022! Beat COVID-19! 1000000000.00\u0026rdquo;\ncheck_element_text(html = html, xpath = \u0026#34;//div[@class=\u0026#39;class1 class2\u0026#39;]\u0026#34;) [1] \u0026ldquo;Happy Valentine\u0026rsquo;s Day! Let\u0026rsquo;s romantic! 1000.0 Happy Lantern Festival! Sweet Dumpling YYDS! 100.0\u0026rdquo;\nExample 8:Asterisk css selector xpath check_element_text(html = html, css = \u0026#34;*.class3\u0026#34;) [1] \u0026ldquo;Welcome 2022! Beat COVID-19! 1000000000.00\u0026rdquo; [2] \u0026ldquo;Happy Spring Festival! Dumpling YYDS! 100.00\u0026rdquo;\n[3] \u0026ldquo;Happy Valentine\u0026rsquo;s Day! Let\u0026rsquo;s romantic! 1000.0\u0026rdquo;\ncheck_element_text(html = html, xpath = \u0026#34;//*[@class=\u0026#39;class3\u0026#39;]\u0026#34;) [1] \u0026ldquo;Welcome 2022! Beat COVID-19! 1000000000.00\u0026rdquo; [2] \u0026ldquo;Happy Spring Festival! Dumpling YYDS! 100.00\u0026rdquo;\n[3] \u0026ldquo;Happy Valentine\u0026rsquo;s Day! Let\u0026rsquo;s romantic! 1000.0\u0026rdquo;\nExample 9:Relationship css selector xpath check_element_text(html = html, css = \u0026#34;div#div2 t\u0026#34;) [1] \u0026ldquo;Dumpling YYDS!\u0026rdquo;\ncheck_element_text(html = html, xpath = \u0026#34;//div[@id=\u0026#39;div2\u0026#39;]/descendant::t\u0026#34;) [1] \u0026ldquo;Dumpling YYDS!\u0026rdquo;\ncheck_element_text(html = html, xpath = \u0026#34;//t[@class=\u0026#39;text1\u0026#39;]/ancestor::div\u0026#34;) [1] \u0026ldquo;Welcome 2022! Beat COVID-19! 1000000000.00\u0026rdquo;\nExample 10:Calculation css selector xpath Not avaiable\ncheck_element_text(html = html, xpath = \u0026#34;//p[price\u0026gt;50.0]/t\u0026#34;) [1] \u0026ldquo;Beat COVID-19!\u0026rdquo; \u0026ldquo;Dumpling YYDS!\u0026rdquo; \u0026ldquo;Let\u0026rsquo;s romantic!\u0026rdquo; \u0026ldquo;Sweet Dumpling YYDS!\u0026rdquo;\n3.3 Summary 3.3.1 CSS Selector Summary Syntax Expression Selector Example Result .class .intro Selects all elements with class=\u0026ldquo;intro\u0026rdquo; .class1.class2 .name1.name2 Selects all elements with both name1 and name2 set within its class attribute .class1 .class2 .name1 .name2 Selects all elements with name2 that is a descendant of an element with name1 #id #firstname Selects the element with id=\u0026ldquo;firstname\u0026rdquo; * * Selects all elements element p Selects all p elements element.class p.intro Selects all p elements with class=\u0026ldquo;intro\u0026rdquo; element,element div, p Selects all div elements and all p elements element element div p Selects all p elements inside div elements element\u0026gt;element div \u0026gt; p Selects all p elements where the parent is a div element element+element div + p Selects the first p element that is placed immediately after div elements element1~element2 p ~ ul Selects every ul element that is preceded by a p element [attribute] [target] Selects all elements with a target attribute [attribute=value] [target=_blank] Selects all elements with target=\u0026quot;_blank\u0026quot; [attribute~=value] [title~=flower] Selects all elements with a title attribute containing the word \u0026ldquo;flower\u0026rdquo; [attribute|=value] [lang|=en] Selects all elements with a lang attribute value equal to \u0026ldquo;en\u0026rdquo; or starting with \u0026ldquo;en-\u0026rdquo; [attribute^=value] a[href^=\u0026ldquo;https\u0026rdquo;] Selects every a element whose href attribute value begins with \u0026ldquo;https\u0026rdquo; [attribute$=value] a[href$=\u0026quot;.pdf\u0026quot;] Selects every a element whose href attribute value ends with \u0026ldquo;.pdf\u0026rdquo; [attribute*=value] a[href*=\u0026ldquo;w3schools\u0026rdquo;] Selects every a element whose href attribute value contains the substring \u0026ldquo;w3schools\u0026rdquo; Selector Example Result :active a:active Selects the active link ::after p::after Insert something after the content of each p element ::before p::before Insert something before¬†the content of each p element :checked input:checked Selects every checked input element :default input:default Selects the default input element :disabled input:disabled Selects every disabled input element :empty p:empty Selects every p element that has no children (including text nodes) :enabled input:enabled Selects every enabled input element :first-child p:first-child Selects every p element that is the first child of its parent ::first-letter p::first-letter Selects the first letter of every p element ::first-line p::first-line Selects the first line of every p element :first-of-type p:first-of-type Selects every p element that is the first p element of its parent :focus input:focus Selects the input element which has focus :fullscreen :fullscreen Selects the element that is in full-screen mode :hover a:hover Selects links on mouse over :in-range input:in-range Selects input elements with a value within a specified range :indeterminate input:indeterminate Selects input elements that are in an indeterminate state :invalid input:invalid Selects all input elements with an invalid value :lang(language) p:lang(it) Selects every p element with a lang attribute equal to \u0026ldquo;it\u0026rdquo; (Italian) :last-child p:last-child Selects every p element that is the last child of its parent :last-of-type p:last-of-type Selects every p element that is the last p element of its parent :link a:link Selects all unvisited links ::marker ::marker Selects the markers of list items :not(selector) :not(p) Selects every element that is not a p element :nth-child(n) p:nth-child(2) Selects every p element that is the second child of its parent :nth-last-child(n) p:nth-last-child(2) Selects every p element that is the second child of its parent, counting from the last child :nth-last-of-type(n) p:nth-last-of-type(2) Selects every p element that is the second p element of its parent, counting from the last child :nth-of-type(n) p:nth-of-type(2) Selects every p element that is the second p element of its parent :only-of-type p:only-of-type Selects every p element that is the only p element of its parent :only-child p:only-child Selects every p element that is the only child of its parent :optional input:optional Selects input elements with no \u0026ldquo;required\u0026rdquo; attribute :out-of-range input:out-of-range Selects input elements with a value outside a specified range ::placeholder input::placeholder Selects input elements with the \u0026ldquo;placeholder\u0026rdquo; attribute specified :read-only input:read-only Selects input elements with the \u0026ldquo;readonly\u0026rdquo; attribute specified :read-write input:read-write Selects input elements with the \u0026ldquo;readonly\u0026rdquo; attribute NOT specified :required input:required Selects input elements with the \u0026ldquo;required\u0026rdquo; attribute specified :root :root Selects the document\u0026rsquo;s root element ::selection ::selection Selects the portion of an element that is selected by a user :target #news:target Selects the current active #news element (clicked on a URL containing that anchor name) :valid input:valid Selects all input elements with a valid value :visited a:visited Selects all visited links CSS Selector Cheat Sheet\n3.3.2 XPATH Summary Syntax Axes Operator Expression Description Example Result nodename Selects all nodes with the name \u0026ldquo;nodename\u0026rdquo; bookstore Selects all nodes with the name \u0026ldquo;bookstore\u0026rdquo; / Selects from the root node /bookstore Selects the root element bookstore // Selects nodes in the document from the current node that match the selection bookstore/book Selects all book elements that are children of bookstore . Selects the current node //book Selects all book elements no matter where they are in the document .. Selects the parent of the current node bookstore//book Selects all book elements that are descendant of the bookstore element @ Selects attributes //@lang Selects all attributes that are named lang * Matches any element node /bookstore/* Selects all the child element nodes of the bookstore element @* Matches any attribute node //* Selects all elements in the document node() Matches any node of any kind //title[@*] Selects all title elements which have at least one attribute of any kind Note: If the path starts with a slash ( / ) it always represents an absolute path to an element!\nAxesname Description Example Result ancestor Selects all ancestors (parent, grandparent, etc.) of the current node ancestor::book Selects all book nodes that are ancestor of the current node ancestor-or-self Selects all ancestors (parent, grandparent, etc.) of the current node and the current node itself attribute Selects all attributes of the current node attribute::* Selects all attributes of the current node child Selects all children of the current node child::*/child::price Selects all price grandchildren of the current node descendant Selects all descendants (children, grandchildren, etc.) of the current node descendant::book Selects all book descendants of the current node descendant-or-self Selects all descendants (children, grandchildren, etc.) of the current node and the current node itself following Selects everything in the document after the closing tag of the current node following::text() Selects all text node that are everything after the current node following-sibling Selects all siblings after the current node following-sibling::node() Selects all siblings after the current node namespace Selects all namespace nodes of the current node parent Selects the parent of the current node parent::book Selects all book nodes that are parent of the current node preceding Selects all nodes that are before the current node, except ancestors, attribute nodes and namespace nodes preceding::price Selects all price nodes that appear before the current node preceding-sibling Selects all siblings before the current node preceding-sibling::book[price\u0026gt;50.0] Selects book nodes with price\u0026gt;50 from siblings before the current node self Selects the current node self::* Selects all in the current node Operator Description Example | Computes two node-sets //book | //cd + Addition 6 + 4 - Subtraction 6 - 4 * Multiplication 6 * 4 div Division 8 div 4 = Equal price=9.80 != Not equal price!=9.80 \u0026lt; Less than price\u0026lt;9.80 \u0026lt;= Less than or equal to price\u0026lt;=9.80 \u0026gt; Greater than price\u0026gt;9.80 \u0026gt;= Greater than or equal to price\u0026gt;=9.80 or or price=9.80 or price=9.70 and and price\u0026gt;9.00 and price\u0026lt;9.90 mod Modulus (division remainder) 5 mod 2 XPATH Cheat Sheet\n"},{"uri":"https://rc-web-crawler.github.io/1st_home/4_demo/","title":"Demo: Web Scraping with R/Python","tags":[],"description":"","content":" Pre-prepare: Include packages that we need python R from bs4 import BeautifulSoup # beautifulsoup4 import requests import pandas as pd import numpy as np import re library(rvest) library(dplyr) Within Novartis, we need proxy to get into some websites.\nPre-prepare: Interact with the website python R url = \u0026#34;https://www.lexjansen.com\u0026#34; proxyDict = {\u0026#39;https\u0026#39;: xxx} # Interact with data via a REST API # Returns a \u0026lt;response\u0026gt; object r = requests.get(url, proxies=proxyDict, verify=False) # for css selector soup = BeautifulSoup(r.text, \u0026#39;lxml\u0026#39;) # for XPath tree = html.fromstring(r.content) # read url website \u0026lt;- \u0026#34;https://www.lexjansen.com\u0026#34; lex \u0026lt;- read_html(website) Different parsers:\n\u0026rsquo;lxml\u0026rsquo;, \u0026lsquo;html.parser\u0026rsquo;, \u0026lsquo;xml\u0026rsquo;\u0026hellip; If it is a perfectly-formed HTML document, small differences between parsers. If the document is not perfectly-formed, different parsers give different results. HTML file \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;!--[if lt IE 7]\u0026gt; \u0026lt;html class=\u0026#34;no-js lt-ie9 lt-ie8 lt-ie7\u0026#34; lang=\u0026#34;en\u0026#34;\u0026gt; \u0026lt;![endif]--\u0026gt;\u0026lt;!--[if IE 7]\u0026gt; \u0026lt;html class=\u0026#34;no-js lt-ie9 lt-ie8\u0026#34; lang=\u0026#34;en\u0026#34;\u0026gt; \u0026lt;![endif]--\u0026gt;\u0026lt;!--[if IE 8]\u0026gt; \u0026lt;html class=\u0026#34;no-js lt-ie9\u0026#34; lang=\u0026#34;en\u0026#34;\u0026gt; \u0026lt;![endif]--\u0026gt;\u0026lt;!--[if gt IE 8]\u0026gt; \u0026lt;html class=\u0026#34;no-js\u0026#34; lang=\u0026#34;en\u0026#34;\u0026gt; \u0026lt;![endif]--\u0026gt;\u0026lt;html\u0026gt;\u0026lt;head\u0026gt; \u0026lt;meta charset=\u0026#34;utf-8\u0026#34;/\u0026gt; \u0026lt;meta content=\u0026#34;IE=edge\u0026#34; http-equiv=\u0026#34;X-UA-Compatible\u0026#34;/\u0026gt; \u0026lt;link href=\u0026#34;https://fonts.googleapis.com/css?family=Special+Elite\u0026amp;amp;v1\u0026#34; rel=\u0026#34;stylesheet\u0026#34; type=\u0026#34;text/css\u0026#34;/\u0026gt; \u0026lt;meta content=\u0026#34;-gdlTJkYvyNRwnnZSHmM94zhCJNRmCwBfdNWk5u2yII\u0026#34; name=\u0026#34;google-site-verification\u0026#34;/\u0026gt; \u0026lt;meta content=\u0026#34;c2f6e02546ad6b1a\u0026#34; name=\u0026#34;yandex-verification\u0026#34;/\u0026gt; \u0026lt;meta content=\u0026#34;telephone=no\u0026#34; name=\u0026#34;format-detection\u0026#34;/\u0026gt; \u0026lt;meta content=\u0026#34;none\u0026#34; name=\u0026#34;msapplication-config\u0026#34;/\u0026gt; \u0026lt;meta content=\u0026#34;SAS Proceedings and more: Fortune Records, Dave Marsh 1001, ...\u0026#34; name=\u0026#34;description\u0026#34;/\u0026gt; \u0026lt;meta content=\u0026#34;SAS Proceedings, Lex Jansen, Fortune Records, The Heart of Rock and Soul, Dave Marsh 1001\u0026#34; name=\u0026#34;KeyWords\u0026#34;/\u0026gt; \u0026lt;title\u0026gt;SAS Proceedings and more\u0026lt;/title\u0026gt; \u0026lt;script type=\u0026#34;application/ld+json\u0026#34;\u0026gt;{\u0026#34;@context\u0026#34;:\u0026#34;https:\\/\\/schema.org\u0026#34;,\u0026#34;@type\u0026#34;:\u0026#34;WebSite\u0026#34;,\u0026#34;url\u0026#34;:\u0026#34;https:\\/\\/www.lexjansen.com\\/\u0026#34;,\u0026#34;name\u0026#34;:\u0026#34;Lex Jansen\u0026#39;s Homepage\u0026#34;}\u0026lt;/script\u0026gt; \u0026lt;script type=\u0026#34;application/ld+json\u0026#34;\u0026gt;{\u0026#34;@context\u0026#34;:\u0026#34;https:\\/\\/schema.org\u0026#34;,\u0026#34;@type\u0026#34;:\u0026#34;Person\u0026#34;,\u0026#34;url\u0026#34;:\u0026#34;https:\\/\\/www.lexjansen.com\\/\u0026#34;, \u0026#34;sameAs\u0026#34;:[\u0026#34;https:\\/\\/instagram.com\\/lex.jansen\\/\u0026#34;,\u0026#34;https:\\/\\/www.evernote.com\\/pub\\/lexjansen\\/public\u0026#34;,\u0026#34;https:\\/\\/www.linkedin.com\\/in\\/lexjansen\u0026#34;,\u0026#34;https:\\/\\/plus.google.com\\/u\\/0\\/+LexJansen\u0026#34;,\u0026#34;https:\\/\\/www.youtube.com\\/user\\/lexjansen\u0026#34;,\u0026#34;https:\\/\\/twitter.com\\/lexjansen\u0026#34;],\u0026#34;name\u0026#34;:\u0026#34;Lex jansen\u0026#34;}\u0026lt;/script\u0026gt; \u0026lt;link href=\u0026#34;favicon.ico\u0026#34; rel=\u0026#34;icon\u0026#34; type=\u0026#34;image/x-icon\u0026#34;/\u0026gt; \u0026lt;link href=\u0026#34;favicon.ico\u0026#34; rel=\u0026#34;shortcut icon\u0026#34; type=\u0026#34;image/x-icon\u0026#34;/\u0026gt; \u0026lt;link href=\u0026#34;/apple-touch-icon.png\u0026#34; rel=\u0026#34;apple-touch-icon\u0026#34;/\u0026gt; \u0026lt;link href=\u0026#34;/apple-touch-icon-57x57.png\u0026#34; rel=\u0026#34;apple-touch-icon\u0026#34; sizes=\u0026#34;57x57\u0026#34;/\u0026gt; ... \u0026lt;br/\u0026gt; Copyright ¬© 1999-2022 Lex Jansen. All rights reserved.\u0026lt;br/\u0026gt; SAS is a registered trademark of SAS Institute Inc. SAS, Statistical Analysis System, and all other SAS Institute Inc. product or service names are registered trademarks or trademarks of SAS Institute Inc. in the USA and other countries. ¬Æ indicates USA registration. \u0026lt;/div\u0026gt; \u0026lt;script src=\u0026#34;https://www.google-analytics.com/urchin.js\u0026#34; type=\u0026#34;text/javascript\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;script type=\u0026#34;text/javascript\u0026#34;\u0026gt; _uacct = \u0026#34;UA-132232-1\u0026#34;; urchinTracker();\u0026lt;/script\u0026gt; \u0026lt;script async=\u0026#34;\u0026#34; data-goatcounter=\u0026#34;https://lexjansen.goatcounter.com/count\u0026#34; src=\u0026#34;//gc.zgo.at/count.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;script type=\u0026#34;text/javascript\u0026#34;\u0026gt; var gaJsHost = ((\u0026#34;https:\u0026#34; == document.location.protocol) ? \u0026#34;https://ssl.\u0026#34; : \u0026#34;https://www.\u0026#34;); document.write(unescape(\u0026#34;%3Cscript src=\u0026#39;\u0026#34; + gaJsHost + \u0026#34;google-analytics.com/ga.js\u0026#39; type=\u0026#39;text/javascript\u0026#39;%3E%3C/script%3E\u0026#34;)); \u0026lt;/script\u0026gt; \u0026lt;script type=\u0026#34;text/javascript\u0026#34;\u0026gt; var pageTracker = _gat._getTracker(\u0026#34;UA-132232-1\u0026#34;); pageTracker._trackPageview(); \u0026lt;/script\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; Structure graph TB; A[Title, Link, Author, Keyword, Pages, Size] B[Conference name, Conference place, Conference time] C[Section name, Best paper flag] F[Final dataset] A --\u003e F B --\u003e F C --\u003e F subgraph g1 [Paper information] A end subgraph g2[Conference information] B end subgraph g3[Paper attributes] C end One example that we use Paper information Title A Case Study of Mining Social Media Data for Disaster Relief: Hurricane Irma Link https://www.sas.com/content/dam/SAS/support/en/sas-global-forum-proceedings/2018/2695-2018.pdf Author Bogdan Gadidov, Linh Le Keyword Text Mining Topic Modeling Time Series Pages 11 Sizes 660 kb Conference information Conference name SAS Global Forum 2018 Conference place Denver, Colorado Conference time April 8-11, 2018 Paper attributes Section name Breakout Best paper flag 1. Homepage -\u0026gt; Get the SUGI url graph LR; A[\"div id='sasproceedings'\"] --\u003e B[ul] B --\u003e C[li]; C --\u003e D[\"div class='pf'\"]; D --\u003e E[div]; E --\u003e F[span] --\u003e G[a]; style G fill:#f3d7d3; \u0026lt;a href=\u0026quot;/sugi\u0026quot;\u0026gt;SUGI / SAS Global Forum\u0026lt;/a\u0026gt; papers (1976-2021)\npython R # CSS selector soup.select(\u0026#34;div[id = \u0026#39;sasproceedings\u0026#39;] \u0026gt; ul \u0026gt; li \u0026gt; div \u0026gt; div \u0026gt; span \u0026gt; a\u0026#34;) soup.select(\u0026#34;div[id = \u0026#39;sasproceedings\u0026#39;] a\u0026#34;) # XPath tree.xpath(\u0026#39;//div[@id =\u0026#34;sasproceedings\u0026#34;]/ul/li/div/div/span/a/text()\u0026#39;) tree.xpath(\u0026#39;//div[@id =\u0026#34;sasproceedings\u0026#34;]/descendant::a/text()\u0026#39;) # Get SUGI URL href = soup.select(\u0026#34;div[id = \u0026#39;sasproceedings\u0026#39;] a\u0026#34;)[10].get(\u0026#39;href\u0026#39;) sugi_url = url + href #sublink1 uri1 \u0026lt;- lex %\u0026gt;% html_nodes(\u0026#34;span a\u0026#34;) %\u0026gt;% html_attr(\u0026#34;href\u0026#34;) sublink1 \u0026lt;- paste0(website,uri1) sublink1[6] conf.ch \u0026lt;- read_html(sublink1[6]) [1] \u0026ldquo;https://www.lexjansen.com/sugi\u0026quot;\n2. SUGI / SAS Global Forum -\u0026gt; Get conference information graph LR; A[\"ul class='conferences'\"] --\u003e B[\"li class='conference'\"] B --\u003e C[a]; B --\u003e D[span]; B --\u003e E[span]; style C fill:#f3d7d3; style D fill:#f3d7d3; style E fill:#f3d7d3; \u0026lt;a href=\u0026quot;../cgi-bin/xsl_transform.php?x=sgf2018\u0026quot;\u0026gt;SAS Global Forum 2018\u0026lt;/a\u0026gt; \u0026lt;span\u0026gt;April 8-11, 2018\u0026lt;/span\u0026gt; \u0026lt;span\u0026gt;Denver, Colorado\u0026lt;/span\u0026gt;\npython R r_sugi = requests.get(sugi_url, proxies=proxyDict, verify=False) soup2 = BeautifulSoup(r_sugi.text, \u0026#39;lxml\u0026#39;) tree2 = html.fromstring(r_sugi.content) # SUGI Forun 2018 url soup2.select(\u0026#39;li a\u0026#39;)[3].get(\u0026#39;href\u0026#39;) sugi_2018_url = url + soup2.select(\u0026#39;li a\u0026#39;)[3].get(\u0026#39;href\u0026#39;)[2:] # 1. conference name # CSS selector soup2.select(\u0026#34;li a\u0026#34;)[3].text # XPath tree2.xpath(\u0026#39;//li/a/text()\u0026#39;)[3] # 2. conference time # CSS selector soup2.select(\u0026#34;li[class = \u0026#39;conference\u0026#39;]\u0026#34;)[3].select(\u0026#34;span\u0026#34;)[0].text # XPath: first \u0026lt;span\u0026gt; element under \u0026lt;li\u0026gt; tree2.xpath(\u0026#39;//li/span[1]/text()\u0026#39;)[3] # 3. conference place # CSS selector soup2.select(\u0026#34;li[class = \u0026#39;conference\u0026#39;]\u0026#34;)[3].select(\u0026#34;span\u0026#34;)[1].text # XPath: second \u0026lt;span\u0026gt; element under \u0026lt;li\u0026gt; tree2.xpath(\u0026#39;//li/span[2]/text()\u0026#39;)[3] # SUGI Forun 2018 link uri2 \u0026lt;- conf.ch %\u0026gt;% html_nodes(\u0026#34;li a\u0026#34;) %\u0026gt;% html_attr(\u0026#34;href\u0026#34;) sublink2 \u0026lt;- paste0(website,substring(uri2,3)) conf.ch.2018 \u0026lt;- read_html(sublink2[4]) # conference name conf.name \u0026lt;- conf.ch %\u0026gt;% html_nodes(\u0026#34;li a\u0026#34;) %\u0026gt;% html_text() (conf.name.f \u0026lt;- conf.name[4]) # conference info (time, place) conf.info \u0026lt;- conf.ch %\u0026gt;% html_nodes(\u0026#34;li span\u0026#34;) %\u0026gt;% html_text() #time conf.time \u0026lt;- conf.info[seq(1,length(conf.info),2)] (conf.time.f \u0026lt;- conf.time[4]) #place conf.place \u0026lt;- conf.info[seq(2,length(conf.info),2)] (conf.place.f \u0026lt;- conf.place[4]) [1] \u0026ldquo;SAS Global Forum 2018\u0026rdquo;\n[1] \u0026ldquo;April 8-11, 2018\u0026rdquo;\n[1] \u0026ldquo;Denver, Colorado\u0026rdquo;\n3. SUGI Forum 2018 webpage -\u0026gt; Get paper information python # create soup3 \u0026amp; tree3 r_sugi_2018 = requests.get(sugi_2018_url, proxies=proxyDict, verify=False) soup3 = BeautifulSoup(r_sugi_2018.text, \u0026#39;lxml\u0026#39;) tree3 = html.fromstring(r_sugi_2018.content) graph LR; A[\"div class='paper wh'\"] --\u003e B[\"a id='sgf2018.2659-2018'\"]; A --\u003e C[\"span class='code'\"]; A --\u003e D[\"a target='_blank'\" Title \u0026 Link]; style D fill:#f3d7d3; A --\u003e E[\"img class='download'\"]; A --\u003e F[a, Author]; A --\u003e G[a, Author]; style F fill:#f3d7d3; style G fill:#f3d7d3; A --\u003e H[\"span class='key'\", keyword] --\u003eI[b]; style H fill:#f3d7d3; A --\u003e J[\"span class='size'\", Pages] --\u003ej[b]; A --\u003e K[\"span xmlns:gcse='uri:dummy-google-ns' class='size'\", Size] --\u003ek[b]; style J fill:#f3d7d3; style K fill:#f3d7d3; Title: Under \u0026lt;a taget=\u0026quot;_blank\u0026quot;\u0026gt; tag\n\u0026lt;a target=\u0026quot;_blank\u0026quot; href=\u0026quot;https://www.sas.com/content/dam/SAS/support/en/sas-global-forum-proceedings/2018/2695-2018.pdf\u0026quot; \u0026gt;A Case Study of Mining Social Media Data for Disaster Relief: Hurricane Irma\u0026lt;/a\u0026gt;\npython R # CSS selector soup3.select(\u0026#39;div.paper \u0026gt; a[target = \u0026#34;_blank\u0026#34;]\u0026#39;)[2].text # XPath: find the second \u0026lt;a\u0026gt; element, which is the child of div.paper.wh tree3.xpath(\u0026#39;//div[contains(@class, \u0026#34;paper\u0026#34;)]/child::a[2]/text()\u0026#39;)[2] # title pname \u0026lt;- conf.ch.2018 %\u0026gt;% html_nodes(xpath=\u0026#34;//*[@target=\u0026#39;_blank\u0026#39;]\u0026#34;) %\u0026gt;% html_text() pname.f \u0026lt;- pname[which(pname != \u0026#34;\u0026#34;)] (pa.name.f \u0026lt;- pname.f[3]) [1] \u0026ldquo;A Case Study of Mining Social Media Data for Disaster Relief: Hurricane Irma\u0026rdquo;\nLink:\npython R # CSS selector soup3.select(\u0026#39;div.paper \u0026gt; a[target = \u0026#34;_blank\u0026#34;]\u0026#39;)[2].get(\u0026#39;href\u0026#39;) # XPath: tree3.xpath(\u0026#39;//div[contains(@class, \u0026#34;paper\u0026#34;)]/child::a[2]/@href\u0026#39;)[2] # paper link filelink \u0026lt;- conf.ch.2018 %\u0026gt;% html_nodes(xpath=\u0026#34;//*[@target=\u0026#39;_blank\u0026#39;]\u0026#34;) %\u0026gt;% html_attr(\u0026#34;href\u0026#34;) filelink.f \u0026lt;- filelink[which(filelink != \u0026#34;\u0026#34;)] (pa.link \u0026lt;- filelink.f[3]) [1] \u0026ldquo;https://www.sas.com/content/dam/SAS/support/en/sas-global-forum-proceedings/2018/2695-2018.pdf\u0026quot;\nAuthor: Under \u0026lt;a\u0026gt; tag\n\u0026lt;a href=\u0026quot;/cgi-bin/xsl_transform.php?x=ag\u0026amp;amp;c=SUGI#bogdidov\u0026quot;\u0026gt;Bogdan Gadidov\u0026lt;/a\u0026gt;\n\u0026lt;a href=\u0026quot;/cgi-bin/xsl_transform.php?x=al\u0026amp;amp;c=SUGI#linhnhle\u0026quot;\u0026gt;Linh Le\u0026lt;/a\u0026gt; python R # beautifulsoup syntax soup3.find_all(\u0026#34;div\u0026#34;, {\u0026#34;class\u0026#34;: \u0026#34;paper\u0026#34;})[2].find_all(\u0026#39;a\u0026#39;, id=None, target=None) # XPath: within the \u0026lt;div\u0026gt;, with attribute contains \u0026#34;paper\u0026#34;, find \u0026lt;a\u0026gt; tag without attributes \u0026#34;id\u0026#34; and \u0026#34;target\u0026#34; tree3.xpath(\u0026#39;//div[contains(@class, \u0026#34;paper\u0026#34;)][3]/a[not(@id) and not(@target)]/text()\u0026#39;) # author author \u0026lt;- conf.ch.2018 %\u0026gt;% html_nodes(xpath=\u0026#34;//div[contains(@class, \u0026#39;paper\u0026#39;)][3]/a[not(@id) and not(@target)]\u0026#34;) %\u0026gt;% html_text() (author.f \u0026lt;- paste0(author[seq(1,length(author),2)],\u0026#34;, \u0026#34;,author[seq(2,length(author),2)])) [1] \u0026ldquo;Bogdan Gadidov, Linh Le\u0026rdquo;\nKeyword: Under \u0026lt;span\u0026gt; tag\n\u0026lt;span class=\u0026quot;key\u0026quot;\u0026gt;\u0026lt;b\u0026gt;Keywords:\u0026lt;/b\u0026gt; Text Mining Topic Modeling Time Series \u0026lt;/span\u0026gt; python R # CSS selector soup3.select(\u0026#39;div.paper \u0026gt; span[class = \u0026#34;key\u0026#34;]\u0026#39;)[0].text # XPath: within the \u0026lt;div\u0026gt;, with attribute contains \u0026#34;paper\u0026#34;, find the \u0026lt;span\u0026gt; tag with attribute class=\u0026#34;key\u0026#34; tree3.xpath(\u0026#39;//div[contains(@class, \u0026#34;paper\u0026#34;)][3]/span[@class = \u0026#34;key\u0026#34;]/text()\u0026#39;) # keyword keyw \u0026lt;- conf.ch.2018 %\u0026gt;% html_nodes(\u0026#34;div.paper span.key\u0026#34;) %\u0026gt;% html_text() keyword \u0026lt;- sub(\u0026#34;.*: \u0026#34;, \u0026#34;\u0026#34;,keyw) (keyword.f \u0026lt;- keyword[1]) [1] \u0026ldquo;Text Mining Topic Modeling Time Series\u0026rdquo;\nPage \u0026amp; Size: Under \u0026lt;span\u0026gt; tag\n\u0026lt;span class=\u0026quot;size\u0026quot;\u0026gt;\u0026lt;b\u0026gt;Pages\u0026lt;/b\u0026gt;:\u0026amp;nbsp;11\u0026amp;nbsp;\u0026lt;/span\u0026gt;\n\u0026lt;span xmlns:gcse=\u0026quot;uri:dummy-google-ns\u0026quot; class=\u0026quot;size\u0026quot;\u0026gt;\u0026lt;b\u0026gt;Size\u0026lt;/b\u0026gt;:\u0026amp;nbsp;660\u0026amp;nbsp;Kb\u0026amp;nbsp;\u0026lt;/span\u0026gt;\npython R # CSS selector soup3.select(\u0026#39;div.paper\u0026#39;)[2].select(\u0026#39;span[class = \u0026#34;size\u0026#34;]\u0026#39;) # XPath: within the \u0026lt;div\u0026gt;, with attribute contains \u0026#34;paper\u0026#34;, find the \u0026lt;span\u0026gt; tag with attribute class=\u0026#34;size\u0026#34; tree3.xpath(\u0026#39;//div[contains(@class, \u0026#34;paper\u0026#34;)][3]/span[@class = \u0026#34;size\u0026#34;]/text()\u0026#39;) # page + size ps \u0026lt;- conf.ch.2018 %\u0026gt;% html_nodes(\u0026#34;div.paper span.size\u0026#34;) %\u0026gt;% html_text() page \u0026lt;- ps[seq(1,length(ps),2)] size \u0026lt;- ps[seq(2,length(ps),2)] (page.f \u0026lt;- page[3]) (size.f \u0026lt;- size[3]) [1] \u0026ldquo;Pages: 11 \u0026quot;\n[1] \u0026ldquo;Size: 660 Kb \u0026quot;\n1. Grab information (Title, Link, Author, Keyword, Pages, Sizes) from one paper # R df \u0026lt;- data.frame(confname=conf.name.f, conftime=conf.time.f, confplace=conf.place.f, title=pa.name.f, link=pa.link, author=author.f, keyword=keyword.f, page=page.f, size=size.f, section=section.f ) library(xlsx) write.xlsx(df, \u0026#34;SUGI_paper.xlsx\u0026#34;) 2. Grab paper information (Title, Link, Author, Keyword, Pages, Sizes) from one conference # python def one_conf(url): r_sugi = requests.get(url, proxies=proxyDict, verify=False) soup = BeautifulSoup(r_sugi.text, \u0026#39;lxml\u0026#39;) num_paper = len(soup.select(\u0026#39;div.paper\u0026#39;)) title = [] link = [] author = [] keyword = [] page = [] size = [] for i in range(num_paper): div = soup.find_all(\u0026#34;div\u0026#34;, {\u0026#34;class\u0026#34;: \u0026#34;paper\u0026#34;})[i] # title \u0026amp; link tl = div.select(\u0026#39;div.paper \u0026gt; a[target = \u0026#34;_blank\u0026#34;]\u0026#39;) if not tl: # tl is empty if div.find(\u0026#34;span\u0026#34;, {\u0026#34;class\u0026#34;: \u0026#34;code\u0026#34;}) is not None: t_new = div.find(\u0026#34;span\u0026#34;, {\u0026#34;class\u0026#34;: \u0026#34;code\u0026#34;}).findNextSibling(text=True) else: t_new = div.find(\u0026#34;a\u0026#34;).findNextSibling(text=True) title.append(t_new) link.append(\u0026#34;\u0026#34;) tag = div.select(\u0026#39;div \u0026gt; a\u0026#39;)[1:] ps = [] else: title.append(tl[0].text) link.append(tl[0].get(\u0026#39;href\u0026#39;)) tag = div.select(\u0026#39;div \u0026gt; a\u0026#39;)[2:] ps = div.find_all(\u0026#34;span\u0026#34;, {\u0026#34;class\u0026#34;: \u0026#34;size\u0026#34;}) # author author2 = [] for j in range(len(tag)): author2.append(tag[j].text) author.append(author2) # keyword key = div.find(\u0026#34;span\u0026#34;, {\u0026#34;class\u0026#34;: \u0026#34;key\u0026#34;}) if key is None: keyword.append(\u0026#34;\u0026#34;) else: keyword.append(key.text[10:]) # page \u0026amp; size if len(ps) \u0026gt;= 2: p = ps[0] s = ps[1] if p is not None and s is not None: if \u0026#39;Page\u0026#39; in p.text: p2 = re.search(\u0026#39;\\xa0(.*)\\xa0\u0026#39;, p.text) page.append(p2.group(1)) if \u0026#39;Size\u0026#39; in s.text: s2_r = re.search(\u0026#39;\\xa0(.*)\\xa0(.*)\\xa0\u0026#39;, s.text) s2 = s2_r.group(1) + \u0026#34; \u0026#34; + s2_r.group(2) size.append(s2) else: size.append(\u0026#34;\u0026#34;) elif \u0026#39;Size\u0026#39; in p.text: s2_r = re.search(\u0026#39;\\xa0(.*)\\xa0(.*)\\xa0\u0026#39;, p.text) s2 = s2_r.group(1) + \u0026#34; \u0026#34; + s2_r.group(2) size.append(s2) page.append(\u0026#34;\u0026#34;) else: size.append(\u0026#34;\u0026#34;) page.append(\u0026#34;\u0026#34;) elif len(ps) == 1: p = ps[0] if \u0026#39;Page\u0026#39; in p.text: p2 = re.search(\u0026#39;\\xa0(.*)\\xa0\u0026#39;, p.text) page.append(p2.group(1)) size.append(\u0026#34;\u0026#34;) elif \u0026#39;Size\u0026#39; in p.text: s2_r = re.search(\u0026#39;\\xa0(.*)\\xa0(.*)\\xa0\u0026#39;, p.text) if s2_r is not None: s2 = s2_r.group(1) + \u0026#34; \u0026#34; + s2_r.group(2) size.append(s2) page.append(\u0026#34;\u0026#34;) else: page.append(\u0026#34;\u0026#34;) size.append(\u0026#34;\u0026#34;) else: page.append(\u0026#34;\u0026#34;) size.append(\u0026#34;\u0026#34;) else: page.append(\u0026#34;\u0026#34;) size.append(\u0026#34;\u0026#34;) # derive dataframe dataset = pd.DataFrame({\u0026#39;Title\u0026#39; : title, \u0026#39;Keyword\u0026#39;: keyword, \u0026#39;Link\u0026#39; : link, \u0026#39;Author\u0026#39; : author, \u0026#39;Pages\u0026#39; : page, \u0026#39;Size\u0026#39; : size}) dataset[\u0026#39;Author\u0026#39;] = [\u0026#34;, \u0026#34;.join(n) for n in dataset[\u0026#39;Author\u0026#39;]] return dataset df = one_conf(url=\u0026#34;https://www.lexjansen.com/cgi-bin/xsl_transform.php?x=sgf2018\u0026#34;) 3. Grab conference information (Conference place \u0026amp; time \u0026amp; name) from all conferences # python def all_paper(url): global conf_name, conf_time, conf_place r_sugi = requests.get(url, proxies=proxyDict, verify=False) soup = BeautifulSoup(r_sugi.text, \u0026#39;lxml\u0026#39;) num_paper = len(soup.select(\u0026#34;li a\u0026#34;)) df_list = [] for i in range(num_paper): # print(i) conf_name = soup.select(\u0026#34;li a\u0026#34;)[i].text conf_time = soup.select(\u0026#34;li \u0026gt; span\u0026#34;)[2*i].text conf_place = soup.select(\u0026#34;li \u0026gt; span\u0026#34;)[2*i+1].text conf_url = \u0026#39;https://www.lexjansen.com\u0026#39; + soup.select(\u0026#39;li a\u0026#39;)[i].get(\u0026#39;href\u0026#39;)[2:] df = one_conf(conf_url) df.insert(column = \u0026#34;Conference_place\u0026#34;, value = conf_place, loc=0) df.insert(column = \u0026#34;Conference_time\u0026#34;, value = conf_time, loc=0) df.insert(column = \u0026#34;Conference_name\u0026#34;, value = conf_name, loc=0) df_list.append(df) return df_list conf = all_paper(\u0026#39;https://www.lexjansen.com/sugi\u0026#39;) SUGI = pd.concat([pd.DataFrame(conf[x]) for x in range(num_paper)], axis = 0, ignore_index=True) SUGI.to_csv(\u0026#39;SUGI.csv\u0026#39;, index = False) graph TB; A[Title, Link, Author, Keyword, Pages, Size] B[Conference name, Conference place, Conference time] C[Section name, Best paper flag] F[One conference] G[One forum] A --grab from 468 papers--\u003e F F --grab from 46 conferences--\u003e G B --merge conference's info --\u003eF C --merge paper attributes --\u003eG subgraph g1 [One paper's info] A end subgraph g2[Conference information] B end subgraph g3[Paper attributes] C end 4. Get paper attributes: Section name Only grab section name \u0026amp; title from the url python R def title_grab(url): r_sugi = requests.get(url, proxies=proxyDict, verify=False) soup = BeautifulSoup(r_sugi.text, \u0026#39;lxml\u0026#39;) num_paper = len(soup.select(\u0026#39;div.paper\u0026#39;)) title = [] for i in range(num_paper): div = soup.find_all(\u0026#34;div\u0026#34;, {\u0026#34;class\u0026#34;: \u0026#34;paper\u0026#34;})[i] # print(i) t = div.find(\u0026#34;a\u0026#34;).findNextSibling() if not t: print(\u0026#34;No title\u0026#34;) else: title.append(t.text) title_df = pd.DataFrame({\u0026#39;Title\u0026#39; : title}) return title_df def section_grab(url): s = requests.get(url, proxies=proxyDict, verify=False) soup = BeautifulSoup(s.text, \u0026#39;lxml\u0026#39;) num_stream = len(soup.select(\u0026#34;div[class=\u0026#39;streams\u0026#39;] span\u0026#34;)) df_list = [] for i in range(num_stream): # print(i) x = soup.find_all(\u0026#34;span\u0026#34;, {\u0026#34;class\u0026#34;: \u0026#34;stream\u0026#34;})[i] sec_name = x.text sec_url = \u0026#39;https://www.lexjansen.com\u0026#39; + x.find(\u0026#34;a\u0026#34;).get(\u0026#39;href\u0026#39;)[2:] df = title_grab(sec_url) df.insert(column = \u0026#34;Section_name\u0026#34;, value = sec_name, loc=0) df_list.append(df) return df_list section = section_grab(\u0026#34;https://www.lexjansen.com/sugi/\u0026#34;) Sections = pd.concat([pd.DataFrame(section[x]) for x in range(num_section)], axis = 0, ignore_index=True) Sections.to_csv(\u0026#39;Sections.csv\u0026#39;, index = False) sct \u0026lt;- conf.ch %\u0026gt;% html_nodes(\u0026#34;div.streams span.stream\u0026#34;) %\u0026gt;% html_text section_name \u0026lt;- sct[7] #get the section links uri3 \u0026lt;- conf.ch %\u0026gt;% html_nodes(\u0026#34;div.streams span.stream a\u0026#34;) %\u0026gt;% html_attr(\u0026#34;href\u0026#34;) sublink3 \u0026lt;- paste0(website, substring(uri3,3)) sugi.section \u0026lt;- read_html(sublink3[7]) #get paper titles within the section sct_papers \u0026lt;- sugi.section %\u0026gt;% html_nodes(\u0026#34;div.paper a\u0026#34;) %\u0026gt;% html_text section.f \u0026lt;- ifelse(pa.name.f %in% sct_papers, section_name, \u0026#34;\u0026#34;) 5. Merge section names back to the origin file sugi = pd.read_excel(\u0026#39;SUGI.xlsx\u0026#39;) section = pd.read_excel(\u0026#39;Sections.xlsx\u0026#39;) pd.merge(sugi, section, on=\u0026#34;Title\u0026#34;, how=\u0026#39;left\u0026#39;) 6. Get paper attributes: Best paper flag Only grab the title from the url, and provide \u0026ldquo;Y\u0026rdquo; to column \u0026ldquo;Best_paper_fl\u0026rdquo;\ndef best_paper_title(url): r_sugi = requests.get(url, proxies=proxyDict, verify=False) soup = BeautifulSoup(r_sugi.text, \u0026#39;lxml\u0026#39;) title = [] best = soup.select(\u0026#34;div.paperback \u0026gt; a\u0026#34;) if best: for pp in best: title.append(pp.text) paper_df = pd.DataFrame({\u0026#39;Title\u0026#39; : title}) return paper_df def best_paper_fl(url): r_sugi = requests.get(url, proxies=proxyDict, verify=False) soup = BeautifulSoup(r_sugi.text, \u0026#39;lxml\u0026#39;) paper = soup.select(\u0026#34;li a\u0026#34;) df_list = [] for i in range(len(paper)): # print(i) conf_url = \u0026#39;https://www.lexjansen.com\u0026#39; + paper[i].get(\u0026#39;href\u0026#39;)[2:] df = best_paper_title(conf_url) df.insert(column = \u0026#34;Best_paper_fl\u0026#34;, value = \u0026#34;Y\u0026#34;, loc=0) df_list.append(df) return df_list best = best_paper_fl(\u0026#34;https://www.lexjansen.com/sugi/\u0026#34;) best = pd.concat([pd.DataFrame(best[x]) for x in range(46)], axis = 0, ignore_index=True) best.to_csv(\u0026#39;Best_paper.csv\u0026#39;, index = False) 7. Derive final dataset: All papers from one forum python sugi = pd.read_excel(\u0026#39;SUGI.xlsx\u0026#39;) section = pd.read_excel(\u0026#39;Sections.xlsx\u0026#39;) best = pd.read_excel(\u0026#39;Best_paper.xlsx\u0026#39;) final = sugi.merge(section, on=\u0026#39;Title\u0026#39;, how=\u0026#39;left\u0026#39;).merge(best, on=\u0026#39;Title\u0026#39;, how=\u0026#39;left\u0026#39;) final.to_csv(\u0026#39;SUGI_paper.csv\u0026#39;, index = False) "},{"uri":"https://rc-web-crawler.github.io/2nd_home/","title":"Content Analysis","tags":[],"description":"","content":"Refresh Corner: Content Analysis This is the webpage for the Content Analysis study groupüëª Team members: Aiwenüëßüèª, Mengyueüë©üèª, Shuyanüë©üèª‚Äçüíº, Songrenüë©üèª‚Äçüî¨, Weijiaüë©üèª‚Äçü¶∞, Yijieüë®üèª‚Äçüíª Content Analysis 2.1 Keyword Analysis 2.2 Content Analysis 2.3 eCRF Index Shiny App "},{"uri":"https://rc-web-crawler.github.io/2nd_home/5_keyword/","title":"Keyword Analysis","tags":[],"description":"","content":"Purpose =======\nIn this section, we will use text mining methods to derive information from the text of keywords. We explore the frequency, coverage, and the relationship between keywords, therefore identifying keywords which are important for our further analysis and model building.\nPre-prepare: install R packages and import data ===============================================\nlibrary(knitr) library(readxl) library(tidyverse) library(tidytext) library(igraph) library(ggraph) library(textstem) library(RSQLite) library(plotly) data \u0026lt;- read_excel(\u0026quot;~/Downloads//paper_keyword.xlsx\u0026quot;) Data transformation ===================\nKeyword cleaning We transform the data into a tibble (tibbles are a modern take on data frames) and add the row number with the column name ‚Äòdocument‚Äô. We clean the data by removing certain special characters and irrelevant information from the ‚Äòkeyword‚Äô column.\ndata2 \u0026lt;- as_tibble(data) %\u0026gt;% mutate(keyword = tolower(str_trim(sub(\u0026quot;((Pages|Size|\\\\().*)|(√Ç|¬Æ)\u0026quot;, \u0026quot;\u0026quot;, keyword), \u0026quot;left\u0026quot;)), document = row_number()) %\u0026gt;% select(document, title, keyword) kable(data2[1:10,]) document title keyword 1 MARKUP: The Power of Choice and Change code from github eric gebhart, sas institute inc. 2 A Tutorial on Reduced Error Logistic Regression updated jsm 2008 paper 3 The SAS Supervisor sas communities page 4 Introduction to the Macro Language macro 5 Unlimiting a Limited Macro Environment macro 6 The Right Approach to Learning PROC TABULATE tabulate 7 SAS Macro Environments: Local and Global macro 8 Introduction to the Macro Language macro 9 The Right Approach to Learning PROC TABULATE tabulate 10 Conquering the Dreaded Macro Error macro Tokenization we need to split the text into individual words (a process called tokenization) and transform it to a tidy data structure (i.e.¬†each word has its own row). To do this, we use tidytext‚Äôs unnest_tokens() function. We remove duplicates so that if a word appeared in a paper multiple times, it will be counted only once. We also remove keywords that contain numbers only (e.g.¬†years).\nStop words Often in text analysis, we will want to remove stop words: Stop words are words that are not useful for an analysis, typically extremely common words such as ‚Äúthe‚Äù, ‚Äúof‚Äù, ‚Äúto‚Äù, and so forth. We remove stop words in our data by using tidytext‚Äôs get_stopwords() function with an anti_join() from the package dplyr.\nLemmatization In our data, there are words such as ‚Äúmacro‚Äù and macros\u0026quot; that means the same but are in different inflected forms. In order to analyze them as a single item, we need to reduce words to their lemma form. Below is an example of lemmatizing ‚Äúbe‚Äù, using textstem‚Äôs lemmatize_words().\nbw \u0026lt;- c('are', 'am', 'being', 'been', 'be') lemmatize_words(bw) ## [1] \u0026quot;be\u0026quot; \u0026quot;be\u0026quot; \u0026quot;be\u0026quot; \u0026quot;be\u0026quot; \u0026quot;be\u0026quot; # tokenization kw \u0026lt;- data2 %\u0026gt;% unnest_tokens(oldword, keyword) %\u0026gt;% # lemmatizing words mutate(word = case_when(length(oldword) \u0026lt; 6 | oldword %in% c('ods','data','mining','learning') ~ oldword, TRUE ~ lemmatize_words(oldword))) %\u0026gt;% distinct() %\u0026gt;% # remove duplicates anti_join(get_stopwords()) %\u0026gt;% # stop words filter(is.na(as.numeric(word))) # remove numbers kw %\u0026gt;% filter(document==1|document==2) %\u0026gt;% kable() document title oldword word 1 MARKUP: The Power of Choice and Change code code 1 MARKUP: The Power of Choice and Change github github 1 MARKUP: The Power of Choice and Change eric eric 1 MARKUP: The Power of Choice and Change gebhart gebhart 1 MARKUP: The Power of Choice and Change sas sas 1 MARKUP: The Power of Choice and Change institute institute 1 MARKUP: The Power of Choice and Change inc inc 2 A Tutorial on Reduced Error Logistic Regression updated update 2 A Tutorial on Reduced Error Logistic Regression jsm jsm 2 A Tutorial on Reduced Error Logistic Regression paper paper Exploratory analysis ====================\nTerm frequency One measure of how important a word may be is its term frequency. Plot words with a frequency greater than 400:\nkw %\u0026gt;% count(word, sort = TRUE) %\u0026gt;% filter(n \u0026gt; 400) %\u0026gt;% mutate(word = reorder(word, n)) %\u0026gt;% ggplot(aes(word, n)) + geom_col(aes()) + xlab(NULL) + scale_y_continuous(expand = c(0, 0)) + coord_flip() + theme_classic(base_size = 12) + labs(title=\u0026quot;Word frequency\u0026quot;, subtitle=\u0026quot;n \u0026gt; 400\u0026quot;)+ theme(plot.title = element_text(lineheight=.8, face=\u0026quot;bold\u0026quot;)) + scale_fill_brewer() Keyword Coverage Analysis We want to analyze the ability of keywords to cover the articles for further analysis. First, sort keywords in descending order of frequency and give the count of keywords. There are total 1821 cleaned keywords.\nkw_clean \u0026lt;- read_excel(\u0026quot;~/Downloads//kw_clean.xlsx\u0026quot;) keyword \u0026lt;- count(kw_clean, word, sort = TRUE) keyword$keyword_count \u0026lt;- seq_len(nrow(keyword)) nrow(keyword) ## [1] 1821 Secondly, calculate each keyword can cover how many articles and exclude duplicates.\nconn \u0026lt;- dbConnect(RSQLite::SQLite(), \u0026quot;:memory:\u0026quot;) dbWriteTable(conn,\u0026quot;Title\u0026quot;,kw_clean) final \u0026lt;- keyword for (i in 1:nrow(keyword)){ each \u0026lt;- keyword[i,] dbWriteTable(conn, \u0026quot;aa\u0026quot;, each, append = TRUE) final[i,'paper_count'] \u0026lt;- dbGetQuery(conn, \u0026quot;SELECT count (distinct title) from Title where word in (select word from aa)\u0026quot;) } dbDisconnect(conn) Thirdly, generate the keyword coverage plot.\na \u0026lt;- ggplot(final) + geom_point(aes(x=keyword_count, y=paper_count)) + geom_label( label=\u0026quot;(200,10685)\u0026quot;, x=200, y=10800, label.padding = unit(0.55, \u0026quot;lines\u0026quot;), label.size = 0.30, vjust = 0, ) + labs( x = \u0026quot;keyword count\u0026quot;, y = \u0026quot;paper count\u0026quot;, title = \u0026quot;Keyword Coverage\u0026quot;) + theme_classic(base_size = 12) ggplotly(a) We can see that as the keywords increase, the more articles are covered. However, when the keywords increase to a certain range, the coverage rate becomes limited.\nThe results show that the most frequent 50 keywords can cover 87% (9826/11222) of the articles. The top 100 can cover 92% (10365/11222) of the articles and the top 200 can cover more than 95% (10685/11222) of the articles.\nTokenization by n-gram We‚Äôve been using the unnest_tokens function to tokenize by word, but we can also use the function to tokenize into consecutive sequences of words, called n-grams. By seeing how often word X is followed by word Y, we can then build a model of the relationships between them.\n# tokenizing by n-gram bigram \u0026lt;- data2 %\u0026gt;% unnest_tokens(bigram, keyword, token = \u0026quot;ngrams\u0026quot;, n = 2) %\u0026gt;% distinct() Now we use tidyr‚Äôs separate(), which splits a column into multiple columns based on a delimiter. This lets us separate it into two columns, ‚Äúword1‚Äù and ‚Äúword2‚Äù, at which point we can remove cases where either is a stop-word.\n# separate words bigram_separated \u0026lt;- bigram %\u0026gt;% separate(bigram, c(\u0026quot;word1\u0026quot;, \u0026quot;word2\u0026quot;), sep = \u0026quot; \u0026quot;) %\u0026gt;% # lemmatizing words mutate(word1 = case_when(length(word1) \u0026lt; 6 | word1 %in% c('ods','data','mining','learning') ~ word1, TRUE ~ lemmatize_words(word1)), word2 = case_when(length(word2) \u0026lt; 6 | word2 %in% c('ods','data','mining','learning') ~ word2, TRUE ~ lemmatize_words(word2))) # filter stop words and NA stopword \u0026lt;- stopwords::stopwords(\u0026quot;en\u0026quot;) bigram_filtered \u0026lt;- bigram_separated %\u0026gt;% filter(!word1 %in% stopword \u0026amp; is.na(as.numeric(word1))) %\u0026gt;% filter(!word2 %in% stopword \u0026amp; is.na(as.numeric(word2))) %\u0026gt;% filter(word1 != word2) # new bigram counts bigram_count \u0026lt;- bigram_filtered %\u0026gt;% count(word1, word2, sort = TRUE) kable(bigram_count[1:10,]) word1 word2 n enterprise guide 380 proc report 375 sas macro 164 sas graph 157 data step 144 data management 116 proc sql 113 data warehouse 112 clinical trial 110 Network analysis We may be interested in visualizing all of the relationships among words simultaneously. As one common visualization, we can arrange the words into a network graph. A graph can be constructed from a tidy object since it has three variables:\nfrom: the node an edge is coming from to: the node an edge is going towards weight: a numeric value associated with each edge We use the graph_from_data_frame() function from the package igraph, which takes a data frame of edges with columns for ‚Äúfrom‚Äù, ‚Äúto‚Äù, and edge attributes (in this case n). Then we use the ggraph package to convert the igraph object into a ggraph with the ggraph() function.\n# filter for only relatively common combinations bigram_graph \u0026lt;- bigram_count %\u0026gt;% filter(n \u0026gt; 35) %\u0026gt;% graph_from_data_frame() # network graph set.seed(999) a \u0026lt;- grid::arrow(type = \u0026quot;closed\u0026quot;, length = unit(.15, \u0026quot;inches\u0026quot;)) ggraph(bigram_graph, layout = \u0026quot;fr\u0026quot;) + geom_edge_link(aes(edge_alpha = n), show.legend = FALSE, arrow = a, end_cap = circle(.07, 'inches')) + geom_node_point(color = \u0026quot;lightblue\u0026quot;, size = 5) + geom_node_text(aes(label = name), vjust = 1, hjust = 1) + theme_void() "},{"uri":"https://rc-web-crawler.github.io/2nd_home/6_content/","title":"Content Analysis","tags":[],"description":"","content":"Title Analysis: Keyword Prediction Although most of the articles have keywords with them, there are still quite a lot of articles without keywords. Meanwhile, some articles may have complicated keywords, which could also increase the difficulty of analysis. To solve this problem, we can try to predict the keywords with title analysis techniques.\nIn the following parts, the keyword prediction using the results from previous keyword analysis will be demonstrated.\nPre-prepare: Include packages and source data We will mainly use the package quanteda for our analysis. Quanteda is an R package for managing and analyzing textual data. It is designed for R users needing to apply natural language processing to texts, from documents to final analysis.\npackage loading library(tidyverse) library(tidytext) library(dplyr) library(XML) library(knitr) library(tm) library(corpus) library(quanteda) library(readxl) library(topicmodels) library(textstem) rm(list = ls()) set.seed(1234) #Load data paper \u0026lt;- read_excel(\u0026quot;~/tm/app-1/new_kw.xlsx\u0026quot;) Pre-prepare: Including data and building dictonary In our analysis, we will use topic-specific dictionaries. Topic-specific dictionaries is a method somehow similar to sentiment analysis. Its aim is to determine the polarity of a text, which could be done by counting terms that were previously assigned to the given categories. With this method, we can categorize the given titles into specific keywords in our dictionary.\nTo use topic-specific dictionaries, the keyword corpus and dictionary need to be built in advance.\nbuild dictionary #Build keyword corpus paper2 \u0026lt;- paper %\u0026gt;% select(title, keyword) %\u0026gt;% mutate(keyword2 = lemmatize_strings(str_to_lower(keyword))) corp_k \u0026lt;- corpus(paper2, text_field = 'keyword2') token_k \u0026lt;- quanteda::tokens(corp_k, remove_numbers = TRUE, remove_punct = TRUE, remove_symbols = TRUE) #Build dictionary based on keyword analysis result dict \u0026lt;- list( output_delivery_system = c('output','delivery','system'), hash_object = c('hash','object'), machine_learing = c('machine','learing'), dictionary_table = c('dictionary','table'), cdisc = c('cdisc','adam','sdtm','xml','domain','send'), multisheet_workbook = c('multi','sheet','workbook','excel'), business_intelligence = c('business','intelligence'), call_execute = c('call','execute'), repeated_measures = c('repeat','measures'), tagset = c('excelxp', 'tageset'), logistic_regression = c('logistic', 'regression'), time_series = c('time','series'), sas_af = c('sas','af'), sas_viya = c('sas','viya'), sas_internet = c('sas','internet'), sas_base = c('sas','base'), sas_graph = c('sas', 'visual', 'analytics','graph'), sas_consult = c('sas', 'consult', 'consultant'), sas_enterprise = c('sas', 'enterprise','guide','miner'), ods = c('ods','rtf','graphics','microsoft','od'), macro = c('macro'), array = c('array'), healthcare = c('healthcare'), format = c('format'), html = c('html'), global_forum = c('global','forum'), proc_sql = c('proc','sql'), proc_report = c('proc','report'), proc_template = c('proc','template','sasgf'), proc_tabulate = c('proc','tabulate'), clinical_trial = c('clinical','trial'), survival_analysis = c('survival','analysis'), data_warehouse = c('datum','warehouse'), data_mining = c('datum','mining'), data_management = c('datum','management'), data_integration = c('datum','integration'), data_quality = c('datum','quality','clean'), data_visualization = c('datum','visualization'), project_management = c('project','management') ) lexicon \u0026lt;- dictionary(dict) Keyword Cleaning We will calculate the keyword coverage from source data first. In this study, the keyword coverage is defined as the percent of papers with any keyword existing after imputation. Keyword cleaning is performed before the calculation.\nkeyword cleaning #Clean Keyword dfm_k \u0026lt;- dfm(token_k) %\u0026gt;% dfm_remove(c(stopwords(\u0026quot;english\u0026quot;),'√¢','sas','datum')) %\u0026gt;% dfm_group(groups = docvars(token_k)[,\u0026quot;title\u0026quot;]) %\u0026gt;% dfm_lookup(dictionary = lexicon) dfm.prop.k \u0026lt;- dfm_weight(dfm_k, scheme = \u0026quot;prop\u0026quot;) df.prop.k \u0026lt;- convert(dfm.prop.k, \u0026quot;data.frame\u0026quot;) ncol_k \u0026lt;- ncol(df.prop.k) for (i in 1:nrow(df.prop.k)){ df.prop.k[i,'max'] \u0026lt;- max(df.prop.k[i,c(seq(2, ncol_k))]) df.prop.k[i,'keyword_cleaned'] \u0026lt;- '' for (j in 2:ncol_k){ if (df.prop.k[i, j] == df.prop.k[i,'max'] \u0026amp; df.prop.k[i,'max'] != 0){ df.prop.k[i,'keyword_cleaned'] \u0026lt;- paste(df.prop.k[i,'keyword_cleaned'], colnames(df.prop.k)[j]) } } } #cleaned keyword coverage paper3 \u0026lt;- paper2 %\u0026gt;% inner_join(df.prop.k, by=c('title' = 'doc_id')) %\u0026gt;% select(title, keyword2, keyword_cleaned) %\u0026gt;% mutate(null = ifelse(keyword_cleaned == '' \u0026amp; grepl('kb', keyword2), 'Y', 'N'), coverage = ifelse(keyword_cleaned == \u0026quot;\u0026quot;, \u0026quot;N\u0026quot;, \u0026quot;Y\u0026quot;)) %\u0026gt;% filter(null == 'N') fail \u0026lt;- paper3 %\u0026gt;% filter(coverage == 'N') %\u0026gt;% group_by(keyword2) %\u0026gt;% summarise(n = n()) %\u0026gt;% arrange(desc(n)) cov_clean \u0026lt;- paper3 %\u0026gt;% group_by(coverage) %\u0026gt;% summarise(n = n()) %\u0026gt;% ungroup() %\u0026gt;% mutate(percentage = round(n/sum(n)*100,2)) %\u0026gt;% mutate(lab1=paste(coverage , n,sep=':'))#Coverage:73.18% of 13072 ggplot(cov_clean, aes(x='',y=n,fill=lab1)) + geom_bar (stat=\u0026quot;identity\u0026quot;, width=1) + coord_polar (\u0026quot;y\u0026quot;, start=0) + geom_text(aes(label = paste0(percentage, \u0026quot;%\u0026quot;)), position = position_stack(vjust=0.5))+ labs(x = NULL, y = NULL, fill = NULL,title=\u0026quot;Coverage of cleaned keywords\u0026quot;) + theme_classic() + theme(axis.line = element_blank(), axis.text = element_blank(), axis.ticks = element_blank() ) + scale_fill_brewer(palette=\u0026quot;Blues\u0026quot;) Approximately 73.18% of the papers have cleaned keyword coverage.\nKeyword Imputation Next, we will use our dictionary to impute the keyword from the titles. Similarly, the coverage of the imputed keyword will be calculated. Also, we will calculate the accuracy of the imputation by comparing the imputed results with cleaning results. The accuracy here is defined as the percentage of papers where the cleaned keywords exist in the imputed keywords for all the papers imputed.\nkeyword imputation #Build title corpus paper4 \u0026lt;- paper3 %\u0026gt;% filter(coverage == 'Y') %\u0026gt;% mutate(title2 = lemmatize_strings(str_to_lower(title))) corp_t \u0026lt;- corpus(paper4, text_field = 'title2') token_t \u0026lt;- quanteda::tokens(corp_t, remove_numbers = TRUE, remove_punct = TRUE, remove_symbols = TRUE) #Impute Keyword dfm_t \u0026lt;- dfm(token_t) %\u0026gt;% dfm_remove(c(stopwords(\u0026quot;english\u0026quot;),'√¢','sas','datum')) %\u0026gt;% dfm_group(groups = docvars(token_t)[,\u0026quot;title\u0026quot;]) %\u0026gt;% dfm_lookup(dictionary = lexicon) dfm.prop.t \u0026lt;- dfm_weight(dfm_t, scheme = \u0026quot;prop\u0026quot;) df.prop.t \u0026lt;- convert(dfm.prop.t, \u0026quot;data.frame\u0026quot;) ncol_t \u0026lt;- ncol(df.prop.t) for (i in 1:nrow(df.prop.t)){ df.prop.t[i,'max'] \u0026lt;- max(df.prop.t[i,c(seq(2, ncol_t))]) df.prop.t[i,'keyword_imputed'] \u0026lt;- '' for (j in 2:ncol_t){ if (df.prop.t[i, j] == df.prop.t[i,'max'] \u0026amp; df.prop.t[i,'max'] != 0){ df.prop.t[i,'keyword_imputed'] \u0026lt;- paste(df.prop.t[i,'keyword_imputed'], colnames(df.prop.t)[j]) } } } #imputed keyword coverage paper5 \u0026lt;- paper4 %\u0026gt;% inner_join(df.prop.t, by=c('title' = 'doc_id')) %\u0026gt;% select(title, keyword_cleaned, keyword_imputed) %\u0026gt;% mutate(coverage = ifelse(keyword_imputed == \u0026quot;\u0026quot;, \u0026quot;N\u0026quot;, \u0026quot;Y\u0026quot;)) cov_imp \u0026lt;- paper5 %\u0026gt;% group_by(coverage) %\u0026gt;% summarise(n = n()) %\u0026gt;% ungroup() %\u0026gt;% mutate(percentage = round(n/sum(n)*100,2)) %\u0026gt;% mutate(lab1=paste(coverage , n,sep=':'))#Coverage:73.18% of 13072 ggplot(cov_imp, aes(x='',y=n,fill=lab1)) + geom_bar (stat=\u0026quot;identity\u0026quot;, width=1) + coord_polar (\u0026quot;y\u0026quot;, start=0) + geom_text(aes(label = paste0(percentage, \u0026quot;%\u0026quot;)), position = position_stack(vjust=0.5))+ labs(x = NULL, y = NULL, fill = NULL, title=\u0026quot;Coverage of imputed keywords\u0026quot;) + theme_classic() + theme(axis.line = element_blank(), axis.text = element_blank(), axis.ticks = element_blank() ) + scale_fill_brewer(palette=\u0026quot;Blues\u0026quot;) #imputed keyword accuracy test \u0026lt;- paper5 %\u0026gt;% filter(coverage == 'Y') %\u0026gt;% mutate(result = keyword_cleaned %in% keyword_imputed, success = sum(result)) cov_acc \u0026lt;- test %\u0026gt;% group_by(coverage) %\u0026gt;% summarise(n = n()) %\u0026gt;% ungroup() %\u0026gt;% mutate(percentage = round(n/sum(n)*100,2)) %\u0026gt;% mutate(lab1=paste(coverage , n,sep=':')) ggplot(cov_acc, aes(x='',y=n,fill=lab1)) + geom_bar (stat=\u0026quot;identity\u0026quot;, width=1) + coord_polar (\u0026quot;y\u0026quot;, start=0) + geom_text(aes(label = paste0(percentage, \u0026quot;%\u0026quot;)), position = position_stack(vjust=0.5))+ labs(x = NULL, y = NULL, fill = NULL,title=\u0026quot;Percent of imputed keywords that exist in cleaned keywords\u0026quot;) + theme_classic() + theme(axis.line = element_blank(), axis.text = element_blank(), axis.ticks = element_blank() ) + scale_fill_manual(values='#71A92C') Approximately 82.1% of the papers have keyword coverage after imputation. Also, we can find that all the imputed keywords are within cleaned keywords.\nKeyword Prediction Finally, we try to use the dictionary to predict the keyword of all the papers we collected.\nkeyword prediction #Prediction paper_all \u0026lt;- read_excel(\u0026quot;paper.xlsx\u0026quot;) paper_pred \u0026lt;- paper_all %\u0026gt;% anti_join(paper5, by = 'title') %\u0026gt;% mutate(title2 = lemmatize_strings(str_to_lower(title))) corp_p \u0026lt;- corpus(paper_pred, text_field = 'title2') token_p \u0026lt;- quanteda::tokens(corp_p, remove_numbers = TRUE, remove_punct = TRUE, remove_symbols = TRUE) #Predict Keyword dfm_p \u0026lt;- dfm(token_p) %\u0026gt;% dfm_remove(c(stopwords(\u0026quot;english\u0026quot;),'√¢','sas','datum')) %\u0026gt;% dfm_group(groups = docvars(token_p)[,\u0026quot;title\u0026quot;]) %\u0026gt;% dfm_lookup(dictionary = lexicon) dfm.prop.p \u0026lt;- dfm_weight(dfm_p, scheme = \u0026quot;prop\u0026quot;) df.prop.p \u0026lt;- convert(dfm.prop.p, \u0026quot;data.frame\u0026quot;) ncol_p \u0026lt;- ncol(df.prop.p) for (i in 1:nrow(df.prop.p)){ df.prop.p[i,'max'] \u0026lt;- max(df.prop.p[i,c(seq(2, ncol_p))]) df.prop.p[i,'keyword_predict'] \u0026lt;- '' for (j in 2:ncol_p){ if (df.prop.p[i, j] == df.prop.p[i,'max'] \u0026amp; df.prop.p[i,'max'] != 0){ df.prop.p[i,'keyword_predict'] \u0026lt;- paste(df.prop.p[i,'keyword_predict'], colnames(df.prop.p)[j]) } } } #predict keyword coverage paper_pred \u0026lt;- paper_pred %\u0026gt;% inner_join(df.prop.p, by=c('title' = 'doc_id')) %\u0026gt;% mutate(coverage = ifelse(keyword_predict == \u0026quot;\u0026quot;, \u0026quot;N\u0026quot;, \u0026quot;Y\u0026quot;)) cov_pred \u0026lt;- paper_pred %\u0026gt;% group_by(coverage) %\u0026gt;% summarise(n = n()) %\u0026gt;% ungroup() %\u0026gt;% mutate(percentage = round(n/sum(n)*100,2)) %\u0026gt;% mutate(lab1=paste(coverage , n,sep=':'))#Coverage:73.18% of 13072 ggplot(cov_pred, aes(x='',y=n,fill=lab1)) + geom_bar (stat=\u0026quot;identity\u0026quot;, width=1) + coord_polar (\u0026quot;y\u0026quot;, start=0) + geom_text(aes(label = paste0(percentage, \u0026quot;%\u0026quot;)), position = position_stack(vjust=0.5))+ labs(x = NULL, y = NULL, fill = NULL,title=\u0026quot;Coverage of predicted keywords for all papers\u0026quot;) + theme_classic() + theme(axis.line = element_blank(), axis.text = element_blank(), axis.ticks = element_blank() ) + scale_fill_brewer(palette=\u0026quot;Blues\u0026quot;) After predicting keywords using our dictionary model, the predicted keyword coverage of all the papers is approximately 53.16%.\nFurther Explorations and conclusion Our exploration indicates that dictionary model could be useful when keywords are not presented. After prediction with the model, the keywords of more than half of the papers could be imputed. Moreover, the dictionary used in our study is relatively crude. With a better dictionary, the coverage could be even higher.\nBesides, we also tried to use Latent Dirichlet Allocation(LDA) to perform a topic modeling analysis. However, because the number of words in a title is relatively small, we did not get a satisfying result.\nOther methods may also be applied in the topic analysis, such as supervised machine learning. If you are interested, have a try!\n"},{"uri":"https://rc-web-crawler.github.io/2nd_home/7_shiny/","title":"eCRF Index Shiny App","tags":[],"description":"","content":"Instruction Users could upload the eCRF pdf file using new template to get the index for eCRF forms and map with SDTM domains. According to alpha test results, the coverage rate is 70% ~ 85% and the accuracy rate is 85% ~ 100%.\nLimitation As expected, mapping results for efficacy-related domains, like QS, ZW, are low in accuracy and coverage, since their eCRF forms are always study specific forms\nShiny App "},{"uri":"https://rc-web-crawler.github.io/","title":"","tags":[],"description":"","content":"Refresh Corner Welcome to our Refresh Corner study group!üòé ~ ü§πüèª‚Äç‚ôÇÔ∏è Enjoy your journey ~ "},{"uri":"https://rc-web-crawler.github.io/categories/","title":"Categories","tags":[],"description":"","content":""},{"uri":"https://rc-web-crawler.github.io/credits/","title":"credits","tags":[],"description":"","content":"References Beautiful soup4 documentation: https://www.crummy.com/software/BeautifulSoup/bs4/doc/ Lexjansen Paper website: https://www.lexjansen.com XPATH cheat sheet: https://devhints.io/xpath CSS selector cheat sheet: https://frontend30.com/css-selectors-cheatsheet/ "},{"uri":"https://rc-web-crawler.github.io/tags/","title":"Tags","tags":[],"description":"","content":""}]